<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="DarkBones"><meta name=description content="A deep dive into the problems of RAG, how I made my own system context-aware, and a step-by-step guide to setting it up locally."><meta name=keywords content=",AI,machine learning,LLMs,retrieval,self-hosting,RAG"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://darkbones.dev/posts/rag-but-i-made-it-smarter/><title>RAG, But I Made it Smarter :: DarkBones
</title><link rel=stylesheet href=/main.min.244183cde1a38e0b08f82c11791181288f9aac1cc9618cd6f4e9e7710c5768ba.css integrity="sha256-JEGDzeGjjgsI+CwReRGBKI+arBzJYYzW9OnncQxXaLo=" crossorigin=anonymous><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content><meta itemprop=name content="RAG, But I Made it Smarter"><meta itemprop=description content="A deep dive into the problems of RAG, how I made my own system context-aware, and a step-by-step guide to setting it up locally."><meta itemprop=datePublished content="2025-02-25T16:55:55+01:00"><meta itemprop=dateModified content="2025-02-25T16:55:55+01:00"><meta itemprop=wordCount content="3610"><meta itemprop=keywords content="Retrieval-Augmented Generation,RAG,local AI,vector databases,context-aware AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="RAG, But I Made it Smarter"><meta name=twitter:description content="A deep dive into the problems of RAG, how I made my own system context-aware, and a step-by-step guide to setting it up locally."><meta property="article:published_time" content="2025-02-25 16:55:55 +0100 +0100"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>hello</span>
<span class=logo__cursor></span></div></a><span class=header__right></span></span></header><div class=content><main class=post><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
17 minutes</p></div><article><h1 class=post-title><a href=https://darkbones.dev/posts/rag-but-i-made-it-smarter/>RAG, But I Made it Smarter</a></h1><div class=post-excerpt>A deep dive into the problems of RAG, how I made my own system context-aware, and a step-by-step guide to setting it up locally.</div><div class=post-content><p><strong>If you&rsquo;re just here for the setup, you can <a href=#setting-it-all-up>skip to the tutorial</a>. If you&rsquo;re not familiar with RAG, you might want to check out <a href=/posts/explaining-rag/>my previous article</a> first.</strong></p><p>Retrieval-Augmented Generation (RAG) is great—until it isn’t. While it helps LLMs pull in external knowledge, it comes with its own headaches. If you&rsquo;ve ever had a RAG system return irrelevant information, mash together unrelated concepts, or confidently misinterpret first-person writing, you&rsquo;ve run into these issues.</p><p>In this article, I’ll walk you through two major problems I encountered while building my own RAG system: <strong>context blindness</strong> (where retrieved chunks lack enough information to be useful) and <strong>first-person confusion</strong> (where the system doesn’t know who “I” refers to). More importantly, I’ll show you how I fixed them—so your RAG system actually understands what it’s retrieving.</p><p>By the end, you’ll have everything you need to <strong>set up your own 100% local, 100% free, context-aware RAG system</strong>, complete with LLMs, a vector database, and automation to tie it all together.</p><h2 id=rag-problem-1-context-blindness>RAG Problem #1: Context Blindness</h2><p>To illustrate the problems with RAG, let&rsquo;s say my vector database holds detailed plots of my favorite movie series. Let&rsquo;s pick two series that couldn&rsquo;t be further from each other: <em>The Matrix</em> and <em>The Lord of the Rings</em>. For illustration purposes, let&rsquo;s say we&rsquo;re using a small chunk size and no overlap.</p><p>We have blindly split our movie plots into chunks and stored them in our vector database, along with their embeddings. We even take a smart approach by splitting it into full sentences rather than arbitrarily splitting at a fixed number of characters.</p><p>Now watch what might happen if you query the vector database with a question, and what it might return.</p><pre tabindex=0><code>Input:  Who forged the one ring of power?
Output: [
  &#34;Sauron forged the One Ring in Mount Doom to control Middle-earth.&#34;,
  &#34;Neo is believed to be the One who will save humanity.&#34;,
  &#34;The One Ring was made to enslave Elves, Dwarves, and Men.&#34;,
  &#34;Agent Smith wants to destroy both humans and the Matrix.&#34;,
  &#34;Sauron tricked the Elves and secretly made the One Ring.&#34;
]
</code></pre><pre tabindex=0><code>Input:  Who or what was the Oracle in the Matrix?
Output: [
  &#34;The Oracle is a program in the Matrix that guides Neo on his path.&#34;  
  &#34;Galadriel, the Lady of Lothlórien, offers wisdom and foresight to the Fellowship.&#34;  
  &#34;The Oracle helps humans understand their choices within the Matrix.&#34;  
  &#34;Gandalf is a Maia, sent to Middle-earth to guide and counsel its people.&#34;  
  &#34;The Oracle predicts the coming of the One who will end the war.&#34;  
]
</code></pre><p>If you give this information to an LLM and use it as context, it might end up answering the questions like this:</p><p><strong>Input:</strong> <em>Who forged the one ring of power?</em></p><p><strong>LLM Response:</strong><br><em>The One Ring was forged by Sauron and Agent Smith to control Middle-earth and enslave Elves, Dwarves, and Men. Neo is believed to be the One who will save humanity, ignoring the other races. Sauron tricked the Elves into helping, while Agent Smith just wanted to destroy both humans and the Matrix.</em></p><hr><p><strong>Input:</strong> <em>Who or what was the Oracle in the Matrix?</em></p><p><strong>LLM Response:</strong><br><em>The Oracle is a program in the Matrix that guides Neo on his path, much like Galadriel, the Lady of Lothlórien, who offers wisdom and foresight to the Fellowship. Together, they predict the coming of the One who will end the war, by throwing the Matrix into Mount Doom.</em></p><p><img src=gandalf-mac.png alt="Gandalf Mac"></p><p><strong>What happened here?</strong></p><p><em>Garbage in, garbage out.</em> The vector database isn&rsquo;t designed to be smart. It&rsquo;s designed to take an input vector, sort its own vectors by most-similar, and return the top <code>x</code> items. It doesn&rsquo;t know what either of the movies are. In our second prompt, we even specifically ask about The Matrix. But if you&rsquo;ve never seen the Matrix and don&rsquo;t know the story, how are you supposed to know which of these chunks belongs to which movie? The first chunk correctly answers the question, as there&rsquo;s literally an <em>Oracle</em> in the matrix, so that is the most similar chunk. But an oracle, by concept, offers wisdom and foresight. So, considering what little context the vector database has to work with, it&rsquo;s not <em>that</em> wrong to conclude that Galadriel is also similar.</p><h2 id=rag-problem-2-first-person-perspective-confusion>RAG Problem #2: First Person Perspective Confusion</h2><p>I write a lot in the first person. I write articles, technical documentation, tech talk preparations, emails, and I keep a professional diary with things I&rsquo;m working on and achievements that I accomplished.</p><p>But I also store content written by other people in the first person—articles I find interesting but haven’t read yet, documentation for tools I reference, and other resources. They may be neatly organized into directories and sub-directories, but in the end, they all go into the same knowledge base.</p><p>But given the chunks <em>&ldquo;I designed a robust data anomaly detection system.&rdquo;</em> and <em>&ldquo;I climbed Mount Fuji in the least amount of steps.&rdquo;</em>, how is it supposed to know which one belongs to me? Obviously, it&rsquo;s the former, but a vector database has no way of knowing that.</p><h2 id=solving-context-blindness-by-making-chunks-context-aware>Solving Context Blindness by Making Chunks “Context Aware”</h2><p>It&rsquo;s clear that storing individual chunks isn&rsquo;t going to cut it. Take a random chunk out of even a medium-sized document, and it&rsquo;s hard even for us humans to understand what the context is. So we need to make each chunk <em>context aware</em>.</p><p>Let&rsquo;s take a chunk from our example:</p><blockquote><p>The Oracle is a program in the Matrix that guides Neo on his path.</p></blockquote><p>Here&rsquo;s what a <em>context aware</em> version of the chunk looks like:</p><pre tabindex=0><code>&lt;file_summary&gt;
This file contains a detailed explanation of the Matrix film trilogy.
&lt;/file_summary&gt;

&lt;chunk_summary&gt;
This chunk is about the Oracle and how she relates to Neo.
&lt;/chunk_summary&gt;

&lt;chunk_headers&gt;
# The Matrix, ## The First Movie, ### Notable Characters, #### The Oracle
&lt;/chunk_headers&gt;

&lt;chunk_content&gt;
The Oracle is a program in the Matrix that guides Neo on his path.
&lt;/chunk_content&gt;
</code></pre><p>Even if you&rsquo;ve never heard of The Matrix or The Oracle, this makes it clear how relevant the chunk is to a prompt like: <em>&lsquo;Who or what was the Oracle in the Matrix?&rsquo;</em></p><p><em>But how do we get these summaries and headers?</em></p><p>We ask an LLM to summarize the entire file by providing the first and last few chunks, since intros and conclusions usually carry the strongest contextual clues. Then, using this file summary as a guide, we have the LLM create a chunk-specific summary, ensuring the chunk is tied back to the broader topic. And finally, we extract the markdown headers and include the chunk&rsquo;s content. We put all the pieces together and store it in a separate database field called <em>full_context</em>.</p><ul><li><strong>This <em>full_context</em> field is what we vectorize and embed.</strong></li><li><strong>The original chunk content (augmented by the headers) is what the database returns</strong></li></ul><p>By embedding the chunk along with its broader context, we help the RAG system understand where the chunk fits within the bigger picture. Instead of treating it as an isolated sentence, it now knows it’s part of a document about The Matrix, and that this specific chunk focuses on the Oracle’s role.</p><p>By storing the <em>&ldquo;full context&rdquo;</em> in a separate database field, it allows us to:</p><ol><li>Check if the chunk we&rsquo;re processing is already in the database, so we don&rsquo;t have to ask an LLM to summarize it again.</li><li>Return the original chunk content to keep the resulting prompt as small as possible.</li></ol><p><em>Remember, this is not to benefit the LLM. This is to benefit the vector database and help it figure out where to put pieces of information, so the most relevant information can be returned.</em></p><p>This simple addition of context isn’t just useful for decoding movie plots. In real-world scenarios—like legal document retrieval or technical knowledge bases, this strategy drastically reduces irrelevant results and hallucinations, leading to more accurate and trustworthy responses.</p><h2 id=solving-the-first-person-confusion>Solving the First Person Confusion</h2><p>This solution is quite simple, but a little bit hacky. I have a directory in my knowledge base with my first name. If the chunk processor is processing any file in this directory or any of its sub-directories, I add an additional prompt instructing the LLM to replace all instances of <em>I</em>, <em>me</em>, <em>my</em> with my actual full name and to make it <strong>&ldquo;abundantly clear that this chunk is about me&rdquo;</strong>. And then when my prompt gets sent to the vector database to retrieve relevant chunks, it&rsquo;s also augmented with my name by simply prepending it.</p><blockquote><p>John Doe: What is my PB in the 5k?</p></blockquote><p>Sure, this isn’t the most elegant fix, but it works. And when it comes to RAG systems, simple and effective often beats complex and fragile. By swapping out first-person pronouns with my full name in personal files, the system can now differentiate between my achievements and that travel blog I saved about climbing Mount Fuji.</p><h2 id=setting-it-all-up>Setting it all up</h2><p>To set up this system locally, we need a few ingredients:</p><ul><li>A database: <em>postgres</em></li><li>An LLM interface: <em>ollama</em></li><li>Some LLMs: <em>qwen2.5</em> / <em>deepseek-r1</em> / <em>llama3</em></li><li>An embedder: <em>nomic-embed-text</em></li><li>A way to interact with the LLM: <em>open-webui</em></li><li>A vector database: <em>Supabase</em></li><li>A RAG system: <em>darkrag</em> (my very own)</li><li>A way to bring it all together: <em>n8n</em></li></ul><p>All of these components run in Docker containers. If you are new to Docker, I recommend checking out some beginner guides before proceeding.</p><p>All of my containers run on a custom Docker network called ai-network. This ensures they can communicate with each other without needing manual configuration.</p><p>For my system (Arch Linux), I created daemons in my <code>~/systemd/.config/systemd/user/</code> directory, but you can easily adapt for whatever system you&rsquo;re using, be it Windows, Mac, or a different Linux distribution. I opted for running the containers separately, instead of in a large <code>docker-compose</code> setup, so I can easily enable/disable different components if I need to free up some memory.</p><h3 id=setting-up-ollama-and-downloading-llms>Setting up Ollama and downloading LLMs</h3><p>You can spin up <code>ollama</code> using this command:</p><pre tabindex=0><code>/usr/bin/docker run --rm \
    --network=ai-network \
    --name ollama \
    -v ollama_models:/root/.ollama/models \
    -p 11434:11434 \
    ollama/ollama:latest
</code></pre><p>Important to note that I created a volume named <code>ollama_models</code> and mapped it to <code>/root/.ollama/models</code> inside the container. This ensures that when we shut down the container, we don&rsquo;t have to re-download the models.</p><p>When <strong>ollama</strong> is running, run this command to go into the container:
<code>docker exec -it ollama /bin/bash</code></p><p>Inside the container, run the <code>ollama pull</code> command to pull your desired language models. For this project, I recommend:</p><pre tabindex=0><code>ollama pull qwen2.5
llama pull nomic-embed-text
</code></pre><p>Then run <code>exit</code> to go back to your local machine.</p><p>Personally, I run this docker container in this daemon that starts up automatically when I log into my machine:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-systemd data-lang=systemd><span style=display:flex><span><span style=color:#75715e>; systemd/.config/systemd/user/ollama.service</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Unit]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Description</span><span style=color:#f92672>=</span><span style=color:#e6db74>Ollama AI Model Server</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>After</span><span style=color:#f92672>=</span><span style=color:#e6db74>default.target</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>BindsTo</span><span style=color:#f92672>=</span><span style=color:#e6db74>default.target</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Service]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStartPre</span><span style=color:#f92672>=</span><span style=color:#e6db74>-/usr/bin/docker network create ai-network</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStart</span><span style=color:#f92672>=</span><span style=color:#e6db74>/usr/bin/docker run --rm </span>\
</span></span><span style=display:flex><span><span style=color:#e6db74>    --network=ai-network </span>\
</span></span><span style=display:flex><span><span style=color:#e6db74>    --name ollama </span>\
</span></span><span style=display:flex><span><span style=color:#e6db74>    -v ollama_models:/root/.ollama/models </span>\
</span></span><span style=display:flex><span><span style=color:#e6db74>    -p 11434:11434 </span>\
</span></span><span style=display:flex><span><span style=color:#e6db74>    ollama/ollama:latest</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStop</span><span style=color:#f92672>=</span><span style=color:#e6db74>/usr/bin/docker stop ollama</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStopPost</span><span style=color:#f92672>=</span><span style=color:#e6db74>/usr/bin/docker rm ollama</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Restart</span><span style=color:#f92672>=</span><span style=color:#e6db74>always</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>RestartSec</span><span style=color:#f92672>=</span><span style=color:#e6db74>5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Install]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>WantedBy</span><span style=color:#f92672>=</span><span style=color:#e6db74>default.target</span>
</span></span></code></pre></div><h3 id=setting-up-postgres>Setting up Postgres</h3><p>Several of the components that we are setting up require a database. Spinning up a postgres container is pretty straight-forward. Here&rsquo;s the command to spin up a postgres instance on your machine using docker:</p><pre tabindex=0><code>/usr/bin/docker run --rm \
    --network=ai-network \
    --name=postgres \
    -p 5432:5432 \
    -v ~/Apps/postgres:/var/lib/postgresql/data \
    -e POSTGRES_USER=user \
    -e POSTGRES_PASSWORD=password \
    -e POSTGRES_DB=default_db \
    postgres:16-alpine
</code></pre><p>Just replace <code>user</code> and <code>password</code> with your desired postgres username and password.</p><h3 id=setting-up-open-webui>Setting up Open-Webui</h3><p>Open-webui is a popular web interface for interacting with your local language models. It looks and feels very similar to what you&rsquo;re used to with ChatGPT.</p><p>[screenshot of open-webui]</p><p>You can easily run it with this command:</p><pre tabindex=0><code>/usr/bin/docker run --rm \
    --network=ai-network \
    -e OLLAMA_BASE_URL=http://ollama:11434 \
    -e PORT=4080 \
    -p 4080:4080 \
    -v /mnt/SnapIgnore/AI/ollama/conversations:/app/backend/data \
    --name open-webui \
    ghcr.io/open-webui/open-webui:main
</code></pre><p>I have a directory on my local machine, <code>/mnt/SnapIgnore/AI/ollama/conversations</code> where I store the conversation data, but this can be any folder on your local machine that you prefer.</p><p>You also need to set up a <em>&ldquo;Function&rdquo;</em> in open-webui to intercept your prompts and augment them with data from your knowledge base. In your browser, navigate to <code>http://localhost:4080</code>, then to <code>Settings > Admin Panel > Functions</code> and create a new function. Don&rsquo;t forget to enable the function by clicking on the 3 dots next to the function name, and enabling <em>Global</em> to enable the function for all your model conversations. Alternatively, you can enable the function separately for each model by going into the model settings.</p><p>Here&rsquo;s the function I use in my system:</p><p><a href=https://github.com/DarkBones/darkrag/blob/main/open-webui-webhook-function-example.py>Open-Webui Webhook Function</a></p><h3 id=setting-up-supabase>Setting up Supabase</h3><p>Running <strong>Supabase</strong> locally and configuring it to work with this project is a bit more involved, but should be doable. You can follow their official guide here:</p><p><a href=https://supabase.com/docs/guides/self-hosting/docker>Self-hosting Supabase with Docker</a></p><p>Once you have the <code>docker-compose</code> file for <strong>Supabase</strong>, you need to configure all the services in it to use our <code>ai-network</code> network so our other docker containers can see it. Here&rsquo;s my <code>docker-compose.yml</code> file for <strong>Supabase</strong>:</p><p><a href=https://github.com/DarkBones/darkrag/blob/main/supabase-docker-compose-example.yml>Supabase customized docker-compose</a></p><p>For my own setup, I cloned the <strong>Supabase</strong> repository in my <code>~/Apps</code> directory and run it with this daemon:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-systemd data-lang=systemd><span style=display:flex><span><span style=color:#75715e>; systemd/.config/systemd/user/supabase.service</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Unit]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Description</span><span style=color:#f92672>=</span><span style=color:#e6db74>Supabase</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>After</span><span style=color:#f92672>=</span><span style=color:#e6db74>docker.service</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>BindsTo</span><span style=color:#f92672>=</span><span style=color:#e6db74>default.target</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Service]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStartPre</span><span style=color:#f92672>=</span><span style=color:#e6db74>-/usr/bin/docker network create ai-network || true</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStart</span><span style=color:#f92672>=</span><span style=color:#e6db74>/usr/bin/docker compose -f %h/Apps/supabase/docker/docker-compose.yml up</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>ExecStop</span><span style=color:#f92672>=</span><span style=color:#e6db74>/usr/bin/docker compose -f %h/Apps/supabase/docker/docker-compose.yml down</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>Restart</span><span style=color:#f92672>=</span><span style=color:#e6db74>always</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>RestartSec</span><span style=color:#f92672>=</span><span style=color:#e6db74>5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>[Install]</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>WantedBy</span><span style=color:#f92672>=</span><span style=color:#e6db74>default.target</span>
</span></span></code></pre></div><p>Notice the line where it says <code>ExecStartPre=-/usr/bin/docker network create ai-network || true</code>. This ensures that the docker network <code>ai-network</code> is created. If you try to create a network that already exists, docker throws an error, so appending <code>|| true</code> ensures this is handled gracefully.</p><h4 id=configuring-supabase>Configuring Supabase</h4><p>Once you have <strong>Supabase</strong> set up and running, it&rsquo;s time to configure it. You can go to your <strong>Supabase</strong> instance by navigating to <code>http://localhost:8000</code> in your browser. It will ask you for your username and password, you can find these credentials in the <code>.env</code> file of the <strong>Supabase</strong> repository you downloaded.</p><p>Then, go to <em>SQL editor</em> in the left navigation bar and run this command to create the <code>documents</code> table:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> SEQUENCE documents_id_seq;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> documents (
</span></span><span style=display:flex><span>    id BIGINT <span style=color:#66d9ef>PRIMARY</span> <span style=color:#66d9ef>KEY</span> <span style=color:#66d9ef>DEFAULT</span> nextval(<span style=color:#e6db74>&#39;documents_id_seq&#39;</span>),
</span></span><span style=display:flex><span>    content TEXT,
</span></span><span style=display:flex><span>    metadata JSONB,
</span></span><span style=display:flex><span>    embedding VECTOR(<span style=color:#ae81ff>768</span>),
</span></span><span style=display:flex><span>    content_hash TEXT <span style=color:#66d9ef>DEFAULT</span> <span style=color:#e6db74>&#39;placeholder&#39;</span>::text,
</span></span><span style=display:flex><span>    summary TEXT <span style=color:#66d9ef>DEFAULT</span> <span style=color:#e6db74>&#39;placeholder&#39;</span>::text,
</span></span><span style=display:flex><span>    full_context TEXT <span style=color:#66d9ef>DEFAULT</span> <span style=color:#e6db74>&#39;placeholder&#39;</span>::text
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>You can set up several tables with different names, if you want to have multiple knowledge-bases. My <em>darkrag</em> tool supports having multiple tables.</p><p>Aside from a table to store your embeddings in, you also need a way to query them by similarity. Run this command in the <em>SQL editor</em> to create the function to match embeddings by cosine similarity:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>create</span> <span style=color:#66d9ef>or</span> <span style=color:#66d9ef>replace</span> <span style=color:#66d9ef>function</span> match_documents(
</span></span><span style=display:flex><span>  query_embedding vector,
</span></span><span style=display:flex><span>  match_count int
</span></span><span style=display:flex><span>) <span style=color:#66d9ef>returns</span> <span style=color:#66d9ef>table</span> (
</span></span><span style=display:flex><span>  id uuid,
</span></span><span style=display:flex><span>  content text,
</span></span><span style=display:flex><span>  metadata jsonb,
</span></span><span style=display:flex><span>  similarity double <span style=color:#66d9ef>precision</span>
</span></span><span style=display:flex><span>) <span style=color:#66d9ef>language</span> plpgsql <span style=color:#66d9ef>as</span> <span style=color:#960050;background-color:#1e0010>$$</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> query
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>select</span>
</span></span><span style=display:flex><span>    id,
</span></span><span style=display:flex><span>    content,
</span></span><span style=display:flex><span>    metadata,
</span></span><span style=display:flex><span>    <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> (documents.embedding <span style=color:#f92672>&lt;=&gt;</span> query_embedding) <span style=color:#66d9ef>as</span> similarity
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>from</span> documents
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>order</span> <span style=color:#66d9ef>by</span> documents.embedding <span style=color:#f92672>&lt;=&gt;</span> query_embedding
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>limit</span> match_count;
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>;
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>$$</span>;
</span></span></code></pre></div><p>We also need a way to remove data from the database by the <em>&ldquo;file_path&rdquo;</em> key in the metadata. This way we can avoid having duplicate entries for the same documents. Run this in the <em>SQL editor</em> to create that function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>create</span> <span style=color:#66d9ef>or</span> <span style=color:#66d9ef>replace</span> <span style=color:#66d9ef>function</span> delete_documents_by_path(
</span></span><span style=display:flex><span>  target_path text
</span></span><span style=display:flex><span>) <span style=color:#66d9ef>returns</span> void 
</span></span><span style=display:flex><span><span style=color:#66d9ef>language</span> plpgsql <span style=color:#66d9ef>as</span> <span style=color:#960050;background-color:#1e0010>$$</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>begin</span>
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>delete</span> <span style=color:#66d9ef>from</span> documents 
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>where</span> metadata<span style=color:#f92672>-&gt;&gt;</span><span style=color:#e6db74>&#39;file_path&#39;</span> <span style=color:#f92672>=</span> target_path;
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>;
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>$$</span>;
</span></span></code></pre></div><p>That was&mldr; a lot&mldr; But your <strong>Supabase</strong> instance should be ready now. Let&rsquo;s move onto setting up <em>darkrag</em>.</p><h2 id=setting-up-darkrag>Setting up darkrag</h2><p>By default, <em>darkrag</em> is designed for Markdown files because my knowledge base is mostly structured notes, documentation, and saved articles. However, if you work with other formats—like PDFs, plain text files, or even web pages—you can easily modify the file processing logic to support them.</p><p>I made the <em>darkrag</em> system as easy as possible to set up. First, run this command to pull the image:</p><pre tabindex=0><code>docker pull darkbones/darkrag:latest
</code></pre><p>Then, create an <code>.env</code> file somewhere on your system (E.g. <code>~/Apps/darkrag/.env</code>) and open it with your favorite text editor. Here&rsquo;s an example <code>.env</code> file:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#75715e># .env</span>
</span></span><span style=display:flex><span>DEFAULT_DATABASE_TABLE<span style=color:#f92672>=</span>documents
</span></span><span style=display:flex><span>AUTHOR_NAME<span style=color:#f92672>=</span>John
</span></span><span style=display:flex><span>AUTHOR_FULL_NAME<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;John Doe&#34;</span>
</span></span><span style=display:flex><span>AUTHOR_PRONOUN_ONE<span style=color:#f92672>=</span>he
</span></span><span style=display:flex><span>AUTHOR_PRONOUN_TWO<span style=color:#f92672>=</span>him
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>SUPABASE_URL<span style=color:#f92672>=</span>http://kong:8000
</span></span><span style=display:flex><span>SUPABASE_KEY<span style=color:#f92672>=</span>your-supabase-key-as-found-in-your-supabase-env-file
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>OLLAMA_URL<span style=color:#f92672>=</span>http://ollama:11434
</span></span><span style=display:flex><span>DEFAULT_MODEL<span style=color:#f92672>=</span>qwen2.5:7b
</span></span><span style=display:flex><span>EMBEDDING_MODEL<span style=color:#f92672>=</span>nomic-embed-text:latest
</span></span></code></pre></div><p><em>Important variables:</em></p><ul><li><code>AUTHOR_NAME</code>: For any file in the <code>AUTHOR_NAME</code> directory, or any of its sub-directories, <em>darkrag</em> will prompt the chunk summarizer to replace all first-person references like <em>&ldquo;I&rdquo;</em> or <em>&ldquo;me&rdquo;</em> with your full name, to solve the <em>&ldquo;first-person confusion problem&rdquo;</em>. For example, if a file in <code>your-knowledge-base-directory/John/about-john.md</code> contains <code>I like trains</code>, the chunk summarizer will add something like <em>&ldquo;John Doe likes trains&rdquo;</em> to the contextualized summary of the chunk.</li><li><code>AUTHOR_FULL_NAME</code>: Your full name so <em>darkrag</em> can contextualize first-person chunks.</li><li><code>AUTHOR_PRONOUN_ONE</code> & <code>AUTHOR_PRONOUN_TWO</code>: Needed for the prompt to contextualize first-person chunks.</li><li><code>SUPABASE_KEY</code>: Needed to connect to the <strong>Supabase</strong> instance. You can find this key in your <code>.env</code> file of <strong>Supabase</strong>.</li><li><code>DEFAULT_MODEL</code>: The LLM that will summarize the chunks. I recommend <code>qwen2.5:7b</code> as it&rsquo;s light-weight and accurate enough.</li></ul><p>Replace the variables with actual values as needed.</p><p>Finally, run this command to run <em>darkrag</em>:</p><pre tabindex=0><code>docker run -p 8004:8004 --env-file [path-to-your-env-file] darkbones/darkrag:latest

docker run --rm \
  --network=ai-network
  --name=darkrag
  -v /mnt/SnapIgnore/AI/knowledge:/data
  -p 8004:8004
</code></pre><p>Just remember to replace <code>[path-to-your-env-file]</code> with the actual path to your <code>.env</code> file you created above. If you prefer, you can also provide the environment variables in the <code>docker-run</code> command directly if you prefer that over using an <code>.env</code> file.</p><h3 id=setting-up-n8n>Setting up n8n</h3><p>Now that we have all the individual pieces of the puzzle, it&rsquo;s time to put everything together. If you&rsquo;re not familiar with <em>n8n</em>, it&rsquo;s a powerful, low-code workflow automation tool that allows you to connect various apps, services, and APIs to streamline processes and automate tasks efficiently.</p><p>This is the command I run to spin it up (in a daemon):</p><pre tabindex=0><code>/usr/bin/docker run --rm \
    --network=ai-network \
    --name=n8n \
    -p 5678:5678 \
    -v ~/Apps/n8n:/home/node/.n8n \
    -v /mnt/SnapIgnore/AI/knowledge:/home/knowledge \
    -e N8N_BASIC_AUTH_ACTIVE=true \
    -e N8N_BASIC_AUTH_USER=admin \
    -e N8N_BASIC_AUTH_PASSWORD=yourpassword \
    -e DB_TYPE=postgresdb \
    -e DB_POSTGRESDB_HOST=postgres \
    -e DB_POSTGRESDB_PORT=5432 \
    -e DB_POSTGRESDB_DATABASE=n8n \
    -e DB_POSTGRESDB_USER=user \
    -e DB_POSTGRESDB_PASSWORD=password \
    -e N8N_SECURE_COOKIE=false \
    n8nio/n8n:latest
</code></pre><p><em>Important components:</em></p><ul><li><code>-v</code> maps a directory on your local machine (can be any directory you wish) to the <code>/home/knowledge</code> directory on <em>n8n</em>. This allows <em>n8n</em> to see the files in that directory so you can automatically update the knowledge base if you add or change any file in this directory.</li><li><code>N8N_BASIC_AUTH_USER</code>: change this to your desired username for <em>n8n</em></li><li><code>N8N_BASIC_AUTH_PASSWORD</code>: change this to your desired password for <em>n8n</em></li><li><code>DB_POSTGRESDB_USER</code>: change this to the username you set up when setting up Postgres</li><li><code>DB_POSTGRESDB_PASSWORD</code>: change this to the password you set up when setting up Postgres</li></ul><h4 id=configuring-n8n>Configuring n8n</h4><p>First, you need to configure some credentials to allow <em>n8n</em> to talk to your other services like <em>ollama</em> and <em>Supabase</em>. Setting them up is pretty easy. In your <em>n8n</em> dashboard (<code>http:localhost:5678</code>), got to the <em>Credentials</em> tab and click on <em>New Credential</em>. Select the credential you want to add from the drop-down and follow the steps.</p><p><img src=ollama-credential.png alt="Ollama Credential"></p><p><img src=supabase-credential.png alt="Supabase Credential"></p><p>I have 3 workflows in <em>n8n</em>:</p><ol><li><em>Knowledge Base Updater</em>: Updates the database whenever I add, change, or delete a file in the knowledge base.</li><li><em>Knowledge Base Rebuilder</em>: Periodically runs through all the files in the knowledge base to make sure the vector database is up to date.</li><li><em>RAG Webhook</em>: Takes a user prompt, and returns the 5 most relevant chunks.</li></ol><p>Let&rsquo;s set them up one-by-one.</p><p><strong>Knowledge Base Updater</strong></p><p>This workflow has 2 sets of three nodes; one set for when a file is added or updated, another for when a file is deleted:</p><p><img src=knowledge-updater-1.png alt="Knowledge Base Updater Overview"></p><p>The file updated entry node is configured like so:</p><p><img src=knowledge-updater-2.png alt="Knowledge Base Updater File Updated Node"></p><p>Remembered how we mapped a directory on our local machine to <code>/home/knowledge</code> in <code>n8n</code>? We&rsquo;re using that here!</p><p>The code in the two <code>Code</code> nodes are identical:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>paths</span> <span style=color:#f92672>=</span> [];
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>item</span> <span style=color:#66d9ef>of</span> <span style=color:#a6e22e>$input</span>.<span style=color:#a6e22e>all</span>()) {
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>paths</span>.<span style=color:#a6e22e>push</span>(<span style=color:#a6e22e>item</span>.<span style=color:#a6e22e>json</span>.<span style=color:#a6e22e>path</span>.<span style=color:#a6e22e>replace</span>(<span style=color:#e6db74>/^\/home\/knowledge\//</span>, <span style=color:#e6db74>&#34;&#34;</span>));
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>return</span> { <span style=color:#a6e22e>paths</span> };
</span></span></code></pre></div><p>It just removes <em>"/home/knowledge/"</em> from the start of the path, to match with what <em>darkrag</em> expects.</p><p>And the two <em>Http Request</em> nodes:</p><p><img src=knowledge-updater-3.png alt="Knowledge Base Updater Update Request"></p><p><img src=knowledge-updater-4.png alt="Knowledge Base Updater Delete Request"></p><p><strong>Knowledge Base Rebuilder</strong></p><p>This runs once a week to make doubly sure all the knowledge in the database is up to date.</p><p><img src=rebuilder-1.png alt="Knowledge Base Rebuilder"></p><p>First, it deletes files from the database that are no longer present in the knowledge base.</p><p><img src=rebuilder-2.png alt="Knowledge Base Rebuilder Clean Database"></p><p>Then, it processes all files in the knowledge base to make sure all the information in the knowledge base is also in the database.</p><p><img src=rebuilder-3.png alt="Knowledge Base Rebuilder Process All"></p><p><strong>RAG Webhook</strong></p><p>Finally, we configure our actual webhook. It&rsquo;s another pretty simple <em>n8n</em> workflow:</p><p><img src=webhook-1.png alt="RAG Webhook Overview"></p><p>It contains only a couple of nodes. Here&rsquo;s the configuration for <strong>Supabase</strong>:</p><p><img src=webhook-2.png alt="RAG Webhook Supabase"></p><p><img src=webhook-3.png alt="RAG Webhook Ollama"></p><p>And the code inside the <em>Code</em> node that extracts the knowledge and prepares the request to <strong>darkrag</strong>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>knowledge</span> <span style=color:#f92672>=</span> [];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>item</span> <span style=color:#66d9ef>of</span> <span style=color:#a6e22e>$input</span>.<span style=color:#a6e22e>all</span>()) {
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>relPath</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>item</span>.<span style=color:#a6e22e>json</span>.document.<span style=color:#a6e22e>metadata</span>.<span style=color:#a6e22e>file_path</span>.<span style=color:#a6e22e>replace</span>(<span style=color:#e6db74>/^\/home\/knowledge\//</span>, <span style=color:#e6db74>&#34;&#34;</span>));
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>const</span> <span style=color:#a6e22e>content</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>item</span>.<span style=color:#a6e22e>json</span>.document.<span style=color:#a6e22e>pageContent</span>;
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>knowledge</span>.<span style=color:#a6e22e>push</span>({
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>file_path</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>relPath</span>,
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>content</span><span style=color:#f92672>:</span> <span style=color:#a6e22e>content</span>,
</span></span><span style=display:flex><span>  });
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>return</span> { <span style=color:#a6e22e>knowledge</span> }
</span></span></code></pre></div><p>And as a final step, return what <em>darkrag</em> responds with:</p><p><img src=webhook-4.png alt="RAG Webhook Respond"></p><h2 id=conclusion>Conclusion</h2><p>That&rsquo;s it! Those are all the ingredients to set up your own 100% local, 100% free <em>RAG System</em>. Now when you interact with your favorite model in <em>open-webui</em> the following happens:</p><ol><li>The <em>open-webui</em> function intercepts your message and sends it to the webhook in <em>n8n</em></li><li><em>n8n</em> sends your input to <em>Supabase</em></li><li><em>Supabase</em> responds with the 5 most relevant chunks</li><li>The <em>open-webui</em> function adds those chunks as context</li><li>The LLM responds more accurately to your input</li></ol><p>And when you add, change, or delete a document from your knowledge base:</p><ol><li><em>n8n</em> sends the file path to <em>darkrag</em></li><li><em>darkrag</em> adds more context by summarizing the chunks, and stores the embeddings of contextualized version, along with the original data, on <em>Supabase</em></li></ol><p><strong>&ldquo;Does <em>darkrag</em> this solve all problems with RAG?&rdquo;</strong></p><p>No, <strong>darkrag</strong> is not a silver bullet. It won’t reliably answer questions like <em>“What happened last week?”</em> or <em>“Rank my achievements by impressiveness.”</em> Instead, it’s a foundation for <em>better chunking and embedding</em>, ensuring retrieved chunks retain the context of their source documents.</p><p>Think of <strong>darkrag</strong> as a <em>starting block</em> for building more advanced RAG agents, not a complete solution. You can take the basic <em>n8n</em> setup from this article and extend it into a truly agentic RAG system.</p></div></article><hr><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<span class=tag><a href=https://darkbones.dev/tags/ai/>AI</a></span>
<span class=tag><a href=https://darkbones.dev/tags/machine-learning/>machine learning</a></span>
<span class=tag><a href=https://darkbones.dev/tags/llms/>LLMs</a></span>
<span class=tag><a href=https://darkbones.dev/tags/retrieval/>retrieval</a></span>
<span class=tag><a href=https://darkbones.dev/tags/self-hosting/>self-hosting</a></span>
<span class=tag><a href=https://darkbones.dev/tags/rag/>RAG</a></span></p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
3610 Words</p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
2025-02-25 15:55</p></div><div class=pagination><div class=pagination__buttons><span class="button next"><a href=https://darkbones.dev/posts/explaining-rag/><span class=button__text>RAG Explained</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer></footer></div><script type=text/javascript src=/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc+cxaJzDdCYbAW0X1G+DgZYvtKFXe6MBex8jUJ2JT25mQx+YjACIng=="></script></body></html>
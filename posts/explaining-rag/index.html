<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content="DarkBones"><meta name=description content="A beginner-friendly breakdown of Retrieval-Augmented Generation (RAG), how it works, and why it makes LLMs more powerful."><meta name=keywords content=",AI,machine learning,LLMs,retrieval,RAG"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://darkbones.dev/posts/explaining-rag/><title>RAG Explained :: DarkBones
</title><link rel=stylesheet href=/main.min.244183cde1a38e0b08f82c11791181288f9aac1cc9618cd6f4e9e7710c5768ba.css integrity="sha256-JEGDzeGjjgsI+CwReRGBKI+arBzJYYzW9OnncQxXaLo=" crossorigin=anonymous><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content><meta itemprop=name content="RAG Explained"><meta itemprop=description content="A beginner-friendly breakdown of Retrieval-Augmented Generation (RAG), how it works, and why it makes LLMs more powerful."><meta itemprop=datePublished content="2025-02-23T12:34:51+01:00"><meta itemprop=dateModified content="2025-02-23T12:34:51+01:00"><meta itemprop=wordCount content="1651"><meta itemprop=keywords content="Retrieval-Augmented Generation,RAG,LLM prompting,vector databases,AI knowledge retrieval"><meta name=twitter:card content="summary"><meta name=twitter:title content="RAG Explained"><meta name=twitter:description content="A beginner-friendly breakdown of Retrieval-Augmented Generation (RAG), how it works, and why it makes LLMs more powerful."><meta property="article:published_time" content="2025-02-23 12:34:51 +0100 +0100"></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>hello</span>
<span class=logo__cursor></span></div></a><span class=header__right></span></span></header><div class=content><main class=post><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
8 minutes</p></div><article><h1 class=post-title><a href=https://darkbones.dev/posts/explaining-rag/>RAG Explained</a></h1><div class=post-excerpt>A beginner-friendly breakdown of Retrieval-Augmented Generation (RAG), how it works, and why it makes LLMs more powerful.</div><div class=post-content><p><em>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by enabling them to retrieve relevant information from external sources before generating a response. Because LLMs rely on static training data and do not update automatically, RAG provides a way to integrate fresh, domain-specific, or private knowledge without requiring costly retraining.</em></p><p><em>In this article, we will explore how RAG works, why it is useful, and how it differs from traditional LLM prompting. Expect oversimplified analogies, light technical deep dives, and a few poorly drawn diagrams to tie everything together.</em></p><h2 id=rag>RAG</h2><p>Large Language Models excel at many tasks. They can code, draft emails, hallucinate ingredients for the perfect sandwich, and even write articles, although I still prefer doing that myself. However, they have a major limitation. They lack real-time knowledge. Because training LLMs is a time-consuming process, they do not &ldquo;know&rdquo; about recent events. If you ask one about last week, it will either display a disclaimer, provide an outdated answer, or generate something completely inaccurate.</p><p>Some LLMs overcome this limitation by retrieving real-time data from external sources before responding. This approach, known as <em>Retrieval-Augmented Generationl (RAG)</em>, allows the model to fetch relevant information and incorporate it into the prompt before generating an answer.</p><h2 id=rag---oversimplified>RAG - Oversimplified</h2><p>But how does RAG actually work? Instead of looking it up ourselves, let’s ask our favorite LLM:
<img src=1-rag-is-a-piece-of-cloth.png alt="Rag is a piece of cloth"></p><p>This is not quite what we were hoping for. No problem, we can ask Bob instead.
<img src=2-bob-rag.png alt="Asking Bob"></p><p>Surprisingly, Bob did not know the answer either, but he was able to retrieve it. Here is what happened:</p><ol><li>We asked Bob about RAG.</li><li>Bob went to the library and asked the librarian for information.</li><li>The librarian pointed him to the right aisle.</li><li>Bob retrieved the information.</li><li>Now Bob sounds like an expert. Thanks, Bob.</li></ol><p>This breakdown reveals that Bob is effectively functioning as a RAG agent. With that insight, let’s explore exactly how a RAG agent operates.</p><h2 id=rag---simplified>RAG - Simplified</h2><p>Let&rsquo;s transform our interaction with Bob into an actual RAG system:</p><ul><li>Bob represents the <strong>RAG system</strong>.</li><li>The librarian acts as an <strong>embedder</strong>.</li><li>The library functions as a <strong>vector database</strong>.</li></ul><p><img src=4-rag-system.png alt="Rag System"></p><p>Instead of prompting the LLM directly, the user interacts with the <em>RAG system</em>. The <em>RAG system</em> then forwards the prompt to the <em>embedder</em>, which converts it into a <em>vector</em>. This <em>vector</em> is a numeric representation of the prompt. The idea is that information with similar meaning will have similar vector representations.</p><p>This vector allows the system to find relevant information in the vector database. When the vector representation of the user&rsquo;s prompt is sent to the database, it retrieves the most relevant matches.</p><p>The <em>RAG system</em> then enhances the user&rsquo;s prompt by including the retrieved information:</p><pre tabindex=0><code>&lt;context&gt;
the information returned from the database
&lt;/context&gt;
&lt;user-prompt&gt;
the user&#39;s original prompt
&lt;/user-prompt&gt;
</code></pre><p>That is the entire process. Retrieve, Augment, and Generate. <strong>RAG</strong>.</p><p>However, the system cannot retrieve information that has not been added to the database. How do we store new data? The process is straightforward. Instead of using the vector to find relevant information, the system stores the data along with its vector representation.</p><p><img src=3-rag-injest.png alt="Insert Knowledge"></p><p>If you were only interested in the big picture, congratulations. You now understand the core concept. However, if you&rsquo;re a fellow neckbeard, let&rsquo;s talk a bit more about vectors and embedders.</p><h2 id=what-is-a-vector>What is a Vector?</h2><p>In simple terms, a vector is a set of coordinates that describe how to move from A to B. Look at this graph:</p><p><img src=5-look-at-this-graph.jpg alt="Look at this Graph"></p><p>This graph has two dimensions. Each point, <strong>A</strong>, <strong>B</strong>, <strong>C</strong>, and <strong>D</strong>, can be described using a two-number coordinate system. The first number tells us how far to move to the right from the origin (0), while the second number tells us how far to move up. To reach <strong>A</strong>, the vector is <code>[3, 7]</code>. To reach <strong>D</strong>, the vector is <code>[3, 0]</code>.</p><p>The same principle applies in three dimensions. To move from your desk to the coffee machine, you must travel a certain distance along the <code>x</code>, <code>y</code>, and <code>z</code> axes, forming a three-number coordinate system.</p><p><img src=6-3d-vector.png alt="3 Dimensions"></p><p>Humans struggle to visualize more than three dimensions, but computers do not have this limitation. The math remains the same. Four dimensions? That requires a <em>four-number</em> coordinate system. One hundred dimensions? That requires a <em>100-number</em> coordinate system.</p><p><img src=this-is-fine.png alt="This is Fine"></p><p>The embedder I use operates in a coordinate system with 768 dimensions. When you have finished trying to visualize that, we can return to simpler, easy-to-draw, two-dimensional graphs.</p><h3 id=why-are-vectors-useful>Why Are Vectors Useful?</h3><p>Vectors by themselves are simply n-dimensional coordinates that represent points in n-dimensional space. Their usefulness comes from the information they represent.</p><p>In the same way, vectors are coordinates not to places, but to information. A specialized LLM, an <em>embedder</em>, is trained on a large corpus of text to figure out similarities and to place these pieces of information somewhere in n-dimensional space such that similar topics tend to be grouped together. Like, when you go to a social event, you&rsquo;re likely to stick with your friends, colleagues, or at least a group of like-minded people.</p><p><img src=7-word2vec.png alt=Word2Vec></p><p>This graph shows how words that are similar in meaning tend to get grouped together in this n-dimensional space. Modern embedders (like <strong>BERT</strong>) don&rsquo;t use single-word embeddings anymore, but generate <em>contextual embeddings</em>.</p><p>The ability to group similar concepts in vector space makes embeddings powerful. However, early embedding models like <em>Word2Vec</em> had a significant limitation that modern models have addressed.</p><h4 id=quick-tech-tangent>Quick Tech Tangent</h4><p>If you&rsquo;ve been working on AI systems for as long as I have, you might be familiar with <strong>Word2Vec</strong>. While groundbreaking when it came out in 2013, it has a major flaw: it assigns a <strong>single vector</strong> to each word, no matter the context.</p><p>Take the word <em>&ldquo;bat&rdquo;</em>.</p><ul><li>Are we talking about the <strong>flying mammal</strong>? Then it should be near <em>&ldquo;mammal&rdquo;</em>, <em>&ldquo;cave&rdquo;</em>, and <em>&ldquo;nocturnal&rdquo;</em>.</li><li>Or do we mean a <strong>baseball bat</strong>? Then it belongs near <em>&ldquo;ball&rdquo;</em>, <em>&ldquo;pitch&rdquo;</em>, and <em>&ldquo;base&rdquo;</em> (but what <em>base</em>? Military?)</li><li>And what if we&rsquo;re in the world of <strong>fiction</strong>? Then <em>&ldquo;bat&rdquo;</em> relates to <em>&ldquo;vampire&rdquo;</em> and <em>&ldquo;transformation&rdquo;</em>.</li></ul><p><strong>Word2Vec</strong> can&rsquo;t tell the difference. It picks one and sticks with it.</p><p>One thing I find particularly fascinating with Word2Vec is that, since words are now represented by numbers, you can actually do arithmetic on them.</p><p>You can make equations like
<code>king - man + woman = queen</code></p><p>It&rsquo;s wild, but it works (most of the time).</p><p><strong>Tangent over.</strong></p><h3 id=how-are-vectors-used>How are Vectors Used?</h3><p>Now that we understand vectors, the next step is straightforward. We embed the information we want the LLM to access, and when we ask a question about that information, the question itself should be close to the relevant content in vector space. The vector database retrieves the <code>n</code> most relevant pieces of content, where <code>n</code> is a configurable number. It also returns the <em>cosine similarity</em> score for each result, indicating how closely the retrieved content matches the query.</p><p>Cosine similarity measures the angle between two vectors. A smaller angle indicates greater similarity, meaning the retrieved data is more relevant to the prompt.</p><p><img src=8-cosine-similarity.png alt="Cosine Similarity"></p><p>In our example, <em>A</em> and <em>B</em> represent the phrases <em>&ldquo;RAG stands for Retrieval Augmented Generation&rdquo;</em> and <em>&ldquo;Hey LLM, tell me about RAG&rdquo;</em>. Since they are closely related, their vectors are similar. If we instead ask <em>&ldquo;Describe an Eclipse&rdquo;</em>, its vector will be far from the others, making it unrelated. However, if <em>&ldquo;RAG stands for Retrieval Augmented Generation&rdquo;</em> is the only entry in the database, it will still be retrieved, even if it is not relevant to the query.</p><h2 id=limitations-of-rag>Limitations of RAG</h2><p>Typically, we do not store and retrieve entire documents in the vector database. If we did, a single large document could easily exceed the context window of the LLM. If the system is configured to return the ten most relevant pieces of information, and each of them is the size of a full article, your computer quickly turns into a space heater. To prevent this, we split the information into chunks of a predefined size, such as <code>1000</code> characters, and we try to keep the sentences and paragraphs intact.</p><p>However, splitting information into chunks introduces a new problem. Just as <em>Word2Vec</em> struggles to determine meaning from a single word, RAG often fails to understand the full context of a single chunk, especially when that chunk is extracted from the middle of a document.</p><p><img src=9-over-rag.png alt=Rag-time></p><p>Here is a problem I encountered recently. I keep a detailed work diary where I document all my professional achievements. It is extremely useful during performance reviews. However, when I ask my RAG system what I achieved at my current company, it confidently includes accomplishments from my previous jobs. Because I write this diary in the first person and also include information from other sources written in the first person, the system cannot distinguish between them. As a result, it starts attributing achievements to me that I had nothing to do with. That is how I realized something was wrong. My system was suddenly telling me about all the interesting things I supposedly did away from the computer, which is impossible since I never leave my desk.</p><p><strong>Want to know how I fixed this mess?</strong> In [the next article], I walk you through how I made my RAG system <strong>context-aware</strong>. You can even steal my code and set it up on your own computer with a couple of commands.</p><p>As I learned firsthand, retrieving information <em>does not guarantee understanding</em>. In <a href=/posts/rag-but-i-made-it-smarter/>my next article</a>, I demonstrate how I made my RAG system <strong>context-aware</strong> so that it can truly comprehend the context of the data it retrieves.</p><h2 id=conclusion>Conclusion</h2><p>RAG makes LLMs more useful by letting them retrieve information they wouldn’t otherwise have access to. But it’s not magic. It comes with its own challenges, from handling context properly to avoiding irrelevant results.</p><p>And as I found out the hard way, just because an AI can fetch information doesn’t mean it always understands it. In the next article, I’ll show you how I made my RAG system <strong>context-aware</strong>—so it actually knows what it’s talking about.</p></div></article><hr><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<span class=tag><a href=https://darkbones.dev/tags/ai/>AI</a></span>
<span class=tag><a href=https://darkbones.dev/tags/machine-learning/>machine learning</a></span>
<span class=tag><a href=https://darkbones.dev/tags/llms/>LLMs</a></span>
<span class=tag><a href=https://darkbones.dev/tags/retrieval/>retrieval</a></span>
<span class=tag><a href=https://darkbones.dev/tags/rag/>RAG</a></span></p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/><polyline points="10 9 9 9 8 9"/></svg>
1651 Words</p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
2025-02-23 11:34</p></div><div class=pagination><div class=pagination__buttons><span class="button previous"><a href=https://darkbones.dev/posts/rag-but-i-made-it-smarter/><span class=button__icon>←</span>
<span class=button__text>RAG, But I Made it Smarter</span>
</a></span><span class="button next"><a href=https://darkbones.dev/posts/one-foot-in-the-unknown/><span class=button__text>One Foot in the Unknown</span>
<span class=button__icon>→</span></a></span></div></div></main></div><footer class=footer></footer></div><script type=text/javascript src=/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc+cxaJzDdCYbAW0X1G+DgZYvtKFXe6MBex8jUJ2JT25mQx+YjACIng=="></script></body></html>
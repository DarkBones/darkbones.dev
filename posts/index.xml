<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on DarkBones</title><link>https://darkbones.dev/posts/</link><description>Recent content in Posts on DarkBones</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 25 Feb 2025 16:55:55 +0100</lastBuildDate><atom:link href="https://darkbones.dev/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>RAG, But I Made it Smarter</title><link>https://darkbones.dev/posts/rag-but-i-made-it-smarter/</link><pubDate>Tue, 25 Feb 2025 16:55:55 +0100</pubDate><guid>https://darkbones.dev/posts/rag-but-i-made-it-smarter/</guid><description>&lt;p>&lt;strong>If you&amp;rsquo;re just here for the setup, you can &lt;a href="#setting-it-all-up">skip to the tutorial&lt;/a>. If you&amp;rsquo;re not familiar with RAG, you might want to check out &lt;a href="https://darkbones.dev/posts/explaining-rag/">my previous article&lt;/a> first.&lt;/strong>&lt;/p>
&lt;p>Retrieval-Augmented Generation (RAG) is great—until it isn’t. While it helps LLMs pull in external knowledge, it comes with its own headaches. If you&amp;rsquo;ve ever had a RAG system return irrelevant information, mash together unrelated concepts, or confidently misinterpret first-person writing, you&amp;rsquo;ve run into these issues.&lt;/p></description><content type="html"><![CDATA[<p><strong>If you&rsquo;re just here for the setup, you can <a href="#setting-it-all-up">skip to the tutorial</a>. If you&rsquo;re not familiar with RAG, you might want to check out <a href="/posts/explaining-rag/">my previous article</a> first.</strong></p>
<p>Retrieval-Augmented Generation (RAG) is great—until it isn’t. While it helps LLMs pull in external knowledge, it comes with its own headaches. If you&rsquo;ve ever had a RAG system return irrelevant information, mash together unrelated concepts, or confidently misinterpret first-person writing, you&rsquo;ve run into these issues.</p>
<p>In this article, I’ll walk you through two major problems I encountered while building my own RAG system: <strong>context blindness</strong> (where retrieved chunks lack enough information to be useful) and <strong>first-person confusion</strong> (where the system doesn’t know who “I” refers to). More importantly, I’ll show you how I fixed them—so your RAG system actually understands what it’s retrieving.</p>
<p>By the end, you’ll have everything you need to <strong>set up your own 100% local, 100% free, context-aware RAG system</strong>, complete with LLMs, a vector database, and automation to tie it all together.</p>
<h2 id="rag-problem-1-context-blindness">RAG Problem #1: Context Blindness</h2>
<p>To illustrate the problems with RAG, let&rsquo;s say my vector database holds detailed plots of my favorite movie series. Let&rsquo;s pick two series that couldn&rsquo;t be further from each other: <em>The Matrix</em> and <em>The Lord of the Rings</em>. For illustration purposes, let&rsquo;s say we&rsquo;re using a small chunk size and no overlap.</p>
<p>We have blindly split our movie plots into chunks and stored them in our vector database, along with their embeddings. We even take a smart approach by splitting it into full sentences rather than arbitrarily splitting at a fixed number of characters.</p>
<p>Now watch what might happen if you query the vector database with a question, and what it might return.</p>
<pre tabindex="0"><code>Input:  Who forged the one ring of power?
Output: [
  &#34;Sauron forged the One Ring in Mount Doom to control Middle-earth.&#34;,
  &#34;Neo is believed to be the One who will save humanity.&#34;,
  &#34;The One Ring was made to enslave Elves, Dwarves, and Men.&#34;,
  &#34;Agent Smith wants to destroy both humans and the Matrix.&#34;,
  &#34;Sauron tricked the Elves and secretly made the One Ring.&#34;
]
</code></pre><pre tabindex="0"><code>Input:  Who or what was the Oracle in the Matrix?
Output: [
  &#34;The Oracle is a program in the Matrix that guides Neo on his path.&#34;  
  &#34;Galadriel, the Lady of Lothlórien, offers wisdom and foresight to the Fellowship.&#34;  
  &#34;The Oracle helps humans understand their choices within the Matrix.&#34;  
  &#34;Gandalf is a Maia, sent to Middle-earth to guide and counsel its people.&#34;  
  &#34;The Oracle predicts the coming of the One who will end the war.&#34;  
]
</code></pre><p>If you give this information to an LLM and use it as context, it might end up answering the questions like this:</p>
<p><strong>Input:</strong> <em>Who forged the one ring of power?</em></p>
<p><strong>LLM Response:</strong><br>
<em>The One Ring was forged by Sauron and Agent Smith to control Middle-earth and enslave Elves, Dwarves, and Men. Neo is believed to be the One who will save humanity, ignoring the other races. Sauron tricked the Elves into helping, while Agent Smith just wanted to destroy both humans and the Matrix.</em></p>
<hr>
<p><strong>Input:</strong> <em>Who or what was the Oracle in the Matrix?</em></p>
<p><strong>LLM Response:</strong><br>
<em>The Oracle is a program in the Matrix that guides Neo on his path, much like Galadriel, the Lady of Lothlórien, who offers wisdom and foresight to the Fellowship. Together, they predict the coming of the One who will end the war, by throwing the Matrix into Mount Doom.</em></p>
<p><img src="gandalf-mac.png" alt="Gandalf Mac"></p>
<p><strong>What happened here?</strong></p>
<p><em>Garbage in, garbage out.</em> The vector database isn&rsquo;t designed to be smart. It&rsquo;s designed to take an input vector, sort its own vectors by most-similar, and return the top <code>x</code> items. It doesn&rsquo;t know what either of the movies are. In our second prompt, we even specifically ask about The Matrix. But if you&rsquo;ve never seen the Matrix and don&rsquo;t know the story, how are you supposed to know which of these chunks belongs to which movie? The first chunk correctly answers the question, as there&rsquo;s literally an <em>Oracle</em> in the matrix, so that is the most similar chunk. But an oracle, by concept, offers wisdom and foresight. So, considering what little context the vector database has to work with, it&rsquo;s not <em>that</em> wrong to conclude that Galadriel is also similar.</p>
<h2 id="rag-problem-2-first-person-perspective-confusion">RAG Problem #2: First Person Perspective Confusion</h2>
<p>I write a lot in the first person. I write articles, technical documentation, tech talk preparations, emails, and I keep a professional diary with things I&rsquo;m working on and achievements that I accomplished.</p>
<p>But I also store content written by other people in the first person—articles I find interesting but haven’t read yet, documentation for tools I reference, and other resources. They may be neatly organized into directories and sub-directories, but in the end, they all go into the same knowledge base.</p>
<p>But given the chunks <em>&ldquo;I designed a robust data anomaly detection system.&rdquo;</em> and <em>&ldquo;I climbed Mount Fuji in the least amount of steps.&rdquo;</em>, how is it supposed to know which one belongs to me? Obviously, it&rsquo;s the former, but a vector database has no way of knowing that.</p>
<h2 id="solving-context-blindness-by-making-chunks-context-aware">Solving Context Blindness by Making Chunks “Context Aware”</h2>
<p>It&rsquo;s clear that storing individual chunks isn&rsquo;t going to cut it. Take a random chunk out of even a medium-sized document, and it&rsquo;s hard even for us humans to understand what the context is. So we need to make each chunk <em>context aware</em>.</p>
<p>Let&rsquo;s take a chunk from our example:</p>
<blockquote>
<p>The Oracle is a program in the Matrix that guides Neo on his path.</p></blockquote>
<p>Here&rsquo;s what a <em>context aware</em> version of the chunk looks like:</p>
<pre tabindex="0"><code>&lt;file_summary&gt;
This file contains a detailed explanation of the Matrix film trilogy.
&lt;/file_summary&gt;

&lt;chunk_summary&gt;
This chunk is about the Oracle and how she relates to Neo.
&lt;/chunk_summary&gt;

&lt;chunk_headers&gt;
# The Matrix, ## The First Movie, ### Notable Characters, #### The Oracle
&lt;/chunk_headers&gt;

&lt;chunk_content&gt;
The Oracle is a program in the Matrix that guides Neo on his path.
&lt;/chunk_content&gt;
</code></pre><p>Even if you&rsquo;ve never heard of The Matrix or The Oracle, this makes it clear how relevant the chunk is to a prompt like: <em>&lsquo;Who or what was the Oracle in the Matrix?&rsquo;</em></p>
<p><em>But how do we get these summaries and headers?</em></p>
<p>We ask an LLM to summarize the entire file by providing the first and last few chunks, since intros and conclusions usually carry the strongest contextual clues. Then, using this file summary as a guide, we have the LLM create a chunk-specific summary, ensuring the chunk is tied back to the broader topic. And finally, we extract the markdown headers and include the chunk&rsquo;s content. We put all the pieces together and store it in a separate database field called <em>full_context</em>.</p>
<ul>
<li><strong>This <em>full_context</em> field is what we vectorize and embed.</strong></li>
<li><strong>The original chunk content (augmented by the headers) is what the database returns</strong></li>
</ul>
<p>By embedding the chunk along with its broader context, we help the RAG system understand where the chunk fits within the bigger picture. Instead of treating it as an isolated sentence, it now knows it’s part of a document about The Matrix, and that this specific chunk focuses on the Oracle’s role.</p>
<p>By storing the <em>&ldquo;full context&rdquo;</em> in a separate database field, it allows us to:</p>
<ol>
<li>Check if the chunk we&rsquo;re processing is already in the database, so we don&rsquo;t have to ask an LLM to summarize it again.</li>
<li>Return the original chunk content to keep the resulting prompt as small as possible.</li>
</ol>
<p><em>Remember, this is not to benefit the LLM. This is to benefit the vector database and help it figure out where to put pieces of information, so the most relevant information can be returned.</em></p>
<p>This simple addition of context isn’t just useful for decoding movie plots. In real-world scenarios—like legal document retrieval or technical knowledge bases, this strategy drastically reduces irrelevant results and hallucinations, leading to more accurate and trustworthy responses.</p>
<h2 id="solving-the-first-person-confusion">Solving the First Person Confusion</h2>
<p>This solution is quite simple, but a little bit hacky. I have a directory in my knowledge base with my first name. If the chunk processor is processing any file in this directory or any of its sub-directories, I add an additional prompt instructing the LLM to replace all instances of <em>I</em>, <em>me</em>, <em>my</em> with my actual full name and to make it <strong>&ldquo;abundantly clear that this chunk is about me&rdquo;</strong>. And then when my prompt gets sent to the vector database to retrieve relevant chunks, it&rsquo;s also augmented with my name by simply prepending it.</p>
<blockquote>
<p>John Doe: What is my PB in the 5k?</p></blockquote>
<p>Sure, this isn’t the most elegant fix, but it works. And when it comes to RAG systems, simple and effective often beats complex and fragile. By swapping out first-person pronouns with my full name in personal files, the system can now differentiate between my achievements and that travel blog I saved about climbing Mount Fuji.</p>
<h2 id="setting-it-all-up">Setting it all up</h2>
<p>To set up this system locally, we need a few ingredients:</p>
<ul>
<li>A database: <em>postgres</em></li>
<li>An LLM interface: <em>ollama</em></li>
<li>Some LLMs: <em>qwen2.5</em> / <em>deepseek-r1</em> / <em>llama3</em></li>
<li>An embedder: <em>nomic-embed-text</em></li>
<li>A way to interact with the LLM: <em>open-webui</em></li>
<li>A vector database: <em>Supabase</em></li>
<li>A RAG system: <em>darkrag</em> (my very own)</li>
<li>A way to bring it all together: <em>n8n</em></li>
</ul>
<p>All of these components run in Docker containers. If you are new to Docker, I recommend checking out some beginner guides before proceeding.</p>
<p>All of my containers run on a custom Docker network called ai-network. This ensures they can communicate with each other without needing manual configuration.</p>
<p>For my system (Arch Linux), I created daemons in my <code>~/systemd/.config/systemd/user/</code> directory, but you can easily adapt for whatever system you&rsquo;re using, be it Windows, Mac, or a different Linux distribution. I opted for running the containers separately, instead of in a large <code>docker-compose</code> setup, so I can easily enable/disable different components if I need to free up some memory.</p>
<h3 id="setting-up-ollama-and-downloading-llms">Setting up Ollama and downloading LLMs</h3>
<p>You can spin up <code>ollama</code> using this command:</p>
<pre tabindex="0"><code>/usr/bin/docker run --rm \
    --network=ai-network \
    --name ollama \
    -v ollama_models:/root/.ollama/models \
    -p 11434:11434 \
    ollama/ollama:latest
</code></pre><p>Important to note that I created a volume named <code>ollama_models</code> and mapped it to <code>/root/.ollama/models</code> inside the container. This ensures that when we shut down the container, we don&rsquo;t have to re-download the models.</p>
<p>When <strong>ollama</strong> is running, run this command to go into the container:
<code>docker exec -it ollama /bin/bash</code></p>
<p>Inside the container, run the <code>ollama pull</code> command to pull your desired language models. For this project, I recommend:</p>
<pre tabindex="0"><code>ollama pull qwen2.5
ollama pull nomic-embed-text
</code></pre><p>Then run <code>exit</code> to go back to your local machine.</p>
<p>Personally, I run this docker container in this daemon that starts up automatically when I log into my machine:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-systemd" data-lang="systemd"><span style="display:flex;"><span><span style="color:#75715e">; systemd/.config/systemd/user/ollama.service</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Unit]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Description</span><span style="color:#f92672">=</span><span style="color:#e6db74">Ollama AI Model Server</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">After</span><span style="color:#f92672">=</span><span style="color:#e6db74">default.target</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">BindsTo</span><span style="color:#f92672">=</span><span style="color:#e6db74">default.target</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Service]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStartPre</span><span style="color:#f92672">=</span><span style="color:#e6db74">-/usr/bin/docker network create ai-network</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStart</span><span style="color:#f92672">=</span><span style="color:#e6db74">/usr/bin/docker run --rm </span>\
</span></span><span style="display:flex;"><span><span style="color:#e6db74">    --network=ai-network </span>\
</span></span><span style="display:flex;"><span><span style="color:#e6db74">    --name ollama </span>\
</span></span><span style="display:flex;"><span><span style="color:#e6db74">    -v ollama_models:/root/.ollama/models </span>\
</span></span><span style="display:flex;"><span><span style="color:#e6db74">    -p 11434:11434 </span>\
</span></span><span style="display:flex;"><span><span style="color:#e6db74">    ollama/ollama:latest</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStop</span><span style="color:#f92672">=</span><span style="color:#e6db74">/usr/bin/docker stop ollama</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStopPost</span><span style="color:#f92672">=</span><span style="color:#e6db74">/usr/bin/docker rm ollama</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Restart</span><span style="color:#f92672">=</span><span style="color:#e6db74">always</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">RestartSec</span><span style="color:#f92672">=</span><span style="color:#e6db74">5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Install]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">WantedBy</span><span style="color:#f92672">=</span><span style="color:#e6db74">default.target</span>
</span></span></code></pre></div><h3 id="setting-up-postgres">Setting up Postgres</h3>
<p>Several of the components that we are setting up require a database. Spinning up a postgres container is pretty straight-forward. Here&rsquo;s the command to spin up a postgres instance on your machine using docker:</p>
<pre tabindex="0"><code>/usr/bin/docker run --rm \
    --network=ai-network \
    --name=postgres \
    -p 5432:5432 \
    -v ~/Apps/postgres:/var/lib/postgresql/data \
    -e POSTGRES_USER=user \
    -e POSTGRES_PASSWORD=password \
    -e POSTGRES_DB=default_db \
    postgres:16-alpine
</code></pre><p>Just replace <code>user</code> and <code>password</code> with your desired postgres username and password.</p>
<h3 id="setting-up-open-webui">Setting up Open-Webui</h3>
<p>Open-webui is a popular web interface for interacting with your local language models. It looks and feels very similar to what you&rsquo;re used to with ChatGPT.</p>
<p>&lt;![TODO &ndash;&gt;
<img src="open-webui.png" alt="open-webui"></p>
<p>You can easily run it with this command:</p>
<pre tabindex="0"><code>/usr/bin/docker run --rm \
    --network=ai-network \
    -e OLLAMA_BASE_URL=http://ollama:11434 \
    -e PORT=4080 \
    -p 4080:4080 \
    -v /mnt/SnapIgnore/AI/ollama/conversations:/app/backend/data \
    --name open-webui \
    ghcr.io/open-webui/open-webui:main
</code></pre><p>I have a directory on my local machine, <code>/mnt/SnapIgnore/AI/ollama/conversations</code> where I store the conversation data, but this can be any directory on your local machine that you prefer.</p>
<p>You also need to set up a <em>&ldquo;Function&rdquo;</em> in open-webui to intercept your prompts and augment them with data from your knowledge base. In your browser, navigate to <code>http://localhost:4080</code>, then to <code>Settings &gt; Admin Panel &gt; Functions</code> and create a new function. Don&rsquo;t forget to enable the function by clicking on the 3 dots next to the function name, and enabling <em>Global</em> to enable the function for all your model conversations. Alternatively, you can enable the function separately for each model by going into the model settings.</p>
<p>Here&rsquo;s the function I use in my system:</p>
<p><a href="https://github.com/DarkBones/darkrag/blob/main/open-webui-webhook-function-example.py">Open-Webui Webhook Function</a></p>
<h3 id="setting-up-supabase">Setting up Supabase</h3>
<p>Running <strong>Supabase</strong> locally and configuring it to work with this project is a bit more involved, but should be doable. You can follow their official guide here:</p>
<p><a href="https://supabase.com/docs/guides/self-hosting/docker">Self-hosting Supabase with Docker</a></p>
<p>Once you have the <code>docker-compose</code> file for <strong>Supabase</strong>, you need to configure all the services in it to use our <code>ai-network</code> network so our other docker containers can see it. Here&rsquo;s my <code>docker-compose.yml</code> file for <strong>Supabase</strong>:</p>
<p><a href="https://github.com/DarkBones/darkrag/blob/main/supabase-docker-compose-example.yml">Supabase customized docker-compose</a></p>
<p>For my own setup, I cloned the <strong>Supabase</strong> repository in my <code>~/Apps</code> directory and run it with this daemon:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-systemd" data-lang="systemd"><span style="display:flex;"><span><span style="color:#75715e">; systemd/.config/systemd/user/supabase.service</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Unit]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Description</span><span style="color:#f92672">=</span><span style="color:#e6db74">Supabase</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">After</span><span style="color:#f92672">=</span><span style="color:#e6db74">docker.service</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">BindsTo</span><span style="color:#f92672">=</span><span style="color:#e6db74">default.target</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Service]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStartPre</span><span style="color:#f92672">=</span><span style="color:#e6db74">-/usr/bin/docker network create ai-network || true</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStart</span><span style="color:#f92672">=</span><span style="color:#e6db74">/usr/bin/docker compose -f %h/Apps/supabase/docker/docker-compose.yml up</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ExecStop</span><span style="color:#f92672">=</span><span style="color:#e6db74">/usr/bin/docker compose -f %h/Apps/supabase/docker/docker-compose.yml down</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Restart</span><span style="color:#f92672">=</span><span style="color:#e6db74">always</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">RestartSec</span><span style="color:#f92672">=</span><span style="color:#e6db74">5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Install]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">WantedBy</span><span style="color:#f92672">=</span><span style="color:#e6db74">default.target</span>
</span></span></code></pre></div><p>Notice the line where it says <code>ExecStartPre=-/usr/bin/docker network create ai-network || true</code>. This ensures that the docker network <code>ai-network</code> is created. If you try to create a network that already exists, docker throws an error, so appending <code>|| true</code> ensures this is handled gracefully.</p>
<h4 id="configuring-supabase">Configuring Supabase</h4>
<p>Once you have <strong>Supabase</strong> set up and running, it&rsquo;s time to configure it. You can go to your <strong>Supabase</strong> instance by navigating to <code>http://localhost:8000</code> in your browser. It will ask you for your username and password, you can find these credentials in the <code>.env</code> file of the <strong>Supabase</strong> repository you downloaded.</p>
<p>Then, go to <em>SQL editor</em> in the left navigation bar and run this command to create the <code>documents</code> table:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> SEQUENCE documents_id_seq;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> documents (
</span></span><span style="display:flex;"><span>    id BIGINT <span style="color:#66d9ef">PRIMARY</span> <span style="color:#66d9ef">KEY</span> <span style="color:#66d9ef">DEFAULT</span> nextval(<span style="color:#e6db74">&#39;documents_id_seq&#39;</span>),
</span></span><span style="display:flex;"><span>    content TEXT,
</span></span><span style="display:flex;"><span>    metadata JSONB,
</span></span><span style="display:flex;"><span>    embedding VECTOR(<span style="color:#ae81ff">768</span>),
</span></span><span style="display:flex;"><span>    content_hash TEXT <span style="color:#66d9ef">DEFAULT</span> <span style="color:#e6db74">&#39;placeholder&#39;</span>::text,
</span></span><span style="display:flex;"><span>    summary TEXT <span style="color:#66d9ef">DEFAULT</span> <span style="color:#e6db74">&#39;placeholder&#39;</span>::text,
</span></span><span style="display:flex;"><span>    full_context TEXT <span style="color:#66d9ef">DEFAULT</span> <span style="color:#e6db74">&#39;placeholder&#39;</span>::text
</span></span><span style="display:flex;"><span>);
</span></span></code></pre></div><p>You can set up several tables with different names, if you want to have multiple knowledge-bases. My <em>darkrag</em> tool supports having multiple tables.</p>
<p>Aside from a table to store your embeddings in, you also need a way to query them by similarity. Run this command in the <em>SQL editor</em> to create the function to match embeddings by cosine similarity:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">create</span> <span style="color:#66d9ef">or</span> <span style="color:#66d9ef">replace</span> <span style="color:#66d9ef">function</span> match_documents(
</span></span><span style="display:flex;"><span>  query_embedding vector,
</span></span><span style="display:flex;"><span>  match_count int
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">returns</span> <span style="color:#66d9ef">table</span> (
</span></span><span style="display:flex;"><span>  id uuid,
</span></span><span style="display:flex;"><span>  content text,
</span></span><span style="display:flex;"><span>  metadata jsonb,
</span></span><span style="display:flex;"><span>  similarity double <span style="color:#66d9ef">precision</span>
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">language</span> plpgsql <span style="color:#66d9ef">as</span> <span style="color:#960050;background-color:#1e0010">$$</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">begin</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> query
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">select</span>
</span></span><span style="display:flex;"><span>    id,
</span></span><span style="display:flex;"><span>    content,
</span></span><span style="display:flex;"><span>    metadata,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> (documents.embedding <span style="color:#f92672">&lt;=&gt;</span> query_embedding) <span style="color:#66d9ef">as</span> similarity
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">from</span> documents
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">order</span> <span style="color:#66d9ef">by</span> documents.embedding <span style="color:#f92672">&lt;=&gt;</span> query_embedding
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">limit</span> match_count;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>;
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$$</span>;
</span></span></code></pre></div><p>We also need a way to remove data from the database by the <em>&ldquo;file_path&rdquo;</em> key in the metadata. This way we can avoid having duplicate entries for the same documents. Run this in the <em>SQL editor</em> to create that function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">create</span> <span style="color:#66d9ef">or</span> <span style="color:#66d9ef">replace</span> <span style="color:#66d9ef">function</span> delete_documents_by_path(
</span></span><span style="display:flex;"><span>  target_path text
</span></span><span style="display:flex;"><span>) <span style="color:#66d9ef">returns</span> void 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">language</span> plpgsql <span style="color:#66d9ef">as</span> <span style="color:#960050;background-color:#1e0010">$$</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">begin</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">delete</span> <span style="color:#66d9ef">from</span> documents 
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">where</span> metadata<span style="color:#f92672">-&gt;&gt;</span><span style="color:#e6db74">&#39;file_path&#39;</span> <span style="color:#f92672">=</span> target_path;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">end</span>;
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">$$</span>;
</span></span></code></pre></div><p>That was&hellip; a lot&hellip; But your <strong>Supabase</strong> instance should be ready now. Let&rsquo;s move onto setting up <em>darkrag</em>.</p>
<h2 id="setting-up-darkrag">Setting up darkrag</h2>
<p>By default, <em>darkrag</em> is designed for Markdown files because my knowledge base is mostly structured notes, documentation, and saved articles. However, if you work with other formats—like PDFs, plain text files, or even web pages—you can easily modify the file processing logic to support them.</p>
<p>I made the <em>darkrag</em> system as easy as possible to set up. First, run this command to pull the image:</p>
<pre tabindex="0"><code>docker pull darkbones/darkrag:latest
</code></pre><p>Then, create an <code>.env</code> file somewhere on your system (E.g. <code>~/Apps/darkrag/.env</code>) and open it with your favorite text editor. Here&rsquo;s an example <code>.env</code> file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#75715e"># .env</span>
</span></span><span style="display:flex;"><span>DEFAULT_DATABASE_TABLE<span style="color:#f92672">=</span>documents
</span></span><span style="display:flex;"><span>AUTHOR_NAME<span style="color:#f92672">=</span>John
</span></span><span style="display:flex;"><span>AUTHOR_FULL_NAME<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;John Doe&#34;</span>
</span></span><span style="display:flex;"><span>AUTHOR_PRONOUN_ONE<span style="color:#f92672">=</span>he
</span></span><span style="display:flex;"><span>AUTHOR_PRONOUN_TWO<span style="color:#f92672">=</span>him
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>SUPABASE_URL<span style="color:#f92672">=</span>http://kong:8000
</span></span><span style="display:flex;"><span>SUPABASE_KEY<span style="color:#f92672">=</span>your-supabase-key-as-found-in-your-supabase-env-file
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>OLLAMA_URL<span style="color:#f92672">=</span>http://ollama:11434
</span></span><span style="display:flex;"><span>DEFAULT_MODEL<span style="color:#f92672">=</span>qwen2.5:7b
</span></span><span style="display:flex;"><span>EMBEDDING_MODEL<span style="color:#f92672">=</span>nomic-embed-text:latest
</span></span></code></pre></div><p><em>Important variables:</em></p>
<ul>
<li><code>AUTHOR_NAME</code>: For any file in the <code>AUTHOR_NAME</code> directory, or any of its sub-directories, <em>darkrag</em> will prompt the chunk summarizer to replace all first-person references like <em>&ldquo;I&rdquo;</em> or <em>&ldquo;me&rdquo;</em> with your full name, to solve the <em>&ldquo;first-person confusion problem&rdquo;</em>. For example, if a file in <code>your-knowledge-base-directory/John/about-john.md</code> contains <code>I like trains</code>, the chunk summarizer will add something like <em>&ldquo;John Doe likes trains&rdquo;</em> to the contextualized summary of the chunk.</li>
<li><code>AUTHOR_FULL_NAME</code>: Your full name so <em>darkrag</em> can contextualize first-person chunks.</li>
<li><code>AUTHOR_PRONOUN_ONE</code> &amp; <code>AUTHOR_PRONOUN_TWO</code>: Needed for the prompt to contextualize first-person chunks.</li>
<li><code>SUPABASE_KEY</code>: Needed to connect to the <strong>Supabase</strong> instance. You can find this key in your <code>.env</code> file of <strong>Supabase</strong>.</li>
<li><code>DEFAULT_MODEL</code>: The LLM that will summarize the chunks. I recommend <code>qwen2.5:7b</code> as it&rsquo;s light-weight and accurate enough.</li>
</ul>
<p>Replace the variables with actual values as needed.</p>
<p>Finally, run this command to run <em>darkrag</em>:</p>
<pre tabindex="0"><code>docker run --rm \
  --env-file [path-to-your-env-file] \
  --network=ai-network \
  --name=darkrag \
  -v /mnt/SnapIgnore/AI/knowledge:/data \
  -p 8004:8004 \
  darkbones/darkrag:latest
</code></pre><p>Just remember to replace <code>[path-to-your-env-file]</code> with the actual path to your <code>.env</code> file you created above. If you prefer, you can also provide the environment variables in the <code>docker-run</code> command directly if you prefer that over using an <code>.env</code> file.</p>
<h3 id="setting-up-n8n">Setting up n8n</h3>
<p>Now that we have all the individual pieces of the puzzle, it&rsquo;s time to put everything together. If you&rsquo;re not familiar with <em>n8n</em>, it&rsquo;s a powerful, low-code workflow automation tool that allows you to connect various apps, services, and APIs to streamline processes and automate tasks efficiently.</p>
<p>This is the command I run to spin it up:</p>
<pre tabindex="0"><code>/usr/bin/docker run --rm \
    --network=ai-network \
    --name=n8n \
    -p 5678:5678 \
    -v ~/Apps/n8n:/home/node/.n8n \
    -v /mnt/SnapIgnore/AI/knowledge:/home/knowledge \
    -e N8N_BASIC_AUTH_ACTIVE=true \
    -e N8N_BASIC_AUTH_USER=admin \
    -e N8N_BASIC_AUTH_PASSWORD=yourpassword \
    -e DB_TYPE=postgresdb \
    -e DB_POSTGRESDB_HOST=postgres \
    -e DB_POSTGRESDB_PORT=5432 \
    -e DB_POSTGRESDB_DATABASE=n8n \
    -e DB_POSTGRESDB_USER=user \
    -e DB_POSTGRESDB_PASSWORD=password \
    -e N8N_SECURE_COOKIE=false \
    n8nio/n8n:latest
</code></pre><p><em>Important components:</em></p>
<ul>
<li><code>-v</code> maps a directory on your local machine (can be any directory you wish) to the <code>/home/knowledge</code> directory on <em>n8n</em>. This allows <em>n8n</em> to see the files in that directory so you can automatically update the knowledge base if you add or change any file in this directory.</li>
<li><code>N8N_BASIC_AUTH_USER</code>: change this to your desired username for <em>n8n</em></li>
<li><code>N8N_BASIC_AUTH_PASSWORD</code>: change this to your desired password for <em>n8n</em></li>
<li><code>DB_POSTGRESDB_USER</code>: change this to the username you set up when setting up Postgres</li>
<li><code>DB_POSTGRESDB_PASSWORD</code>: change this to the password you set up when setting up Postgres</li>
</ul>
<h4 id="configuring-n8n">Configuring n8n</h4>
<p>First, you need to configure some credentials to allow <em>n8n</em> to talk to your other services like <em>ollama</em> and <em>Supabase</em>. Setting them up is pretty easy. In your <em>n8n</em> dashboard (<code>http:localhost:5678</code>), got to the <em>Credentials</em> tab and click on <em>New Credential</em>. Select the credential you want to add from the drop-down and follow the steps.</p>
<p><img src="ollama-credential.png" alt="Ollama Credential"></p>
<p><img src="supabase-credential.png" alt="Supabase Credential"></p>
<p>I have 3 workflows in <em>n8n</em>:</p>
<ol>
<li><em>Knowledge Base Updater</em>: Updates the database whenever I add, change, or delete a file in the knowledge base.</li>
<li><em>Knowledge Base Rebuilder</em>: Periodically runs through all the files in the knowledge base to make sure the vector database is up to date.</li>
<li><em>RAG Webhook</em>: Takes a user prompt, and returns the 5 most relevant chunks.</li>
</ol>
<p>Let&rsquo;s set them up one-by-one.</p>
<p><strong>Knowledge Base Updater</strong></p>
<p>This workflow has 2 sets of three nodes; one set for when a file is added or updated, another for when a file is deleted:</p>
<p><img src="knowledge-updater-1.png" alt="Knowledge Base Updater Overview"></p>
<p>The file updated entry node is configured like so:</p>
<p><img src="knowledge-updater-2.png" alt="Knowledge Base Updater File Updated Node"></p>
<p>Remembered how we mapped a directory on our local machine to <code>/home/knowledge</code> in <code>n8n</code>? We&rsquo;re using that here!</p>
<p>The code in the two <code>Code</code> nodes are identical:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">paths</span> <span style="color:#f92672">=</span> [];
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">item</span> <span style="color:#66d9ef">of</span> <span style="color:#a6e22e">$input</span>.<span style="color:#a6e22e">all</span>()) {
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">paths</span>.<span style="color:#a6e22e">push</span>(<span style="color:#a6e22e">item</span>.<span style="color:#a6e22e">json</span>.<span style="color:#a6e22e">path</span>.<span style="color:#a6e22e">replace</span>(<span style="color:#e6db74">/^\/home\/knowledge\//</span>, <span style="color:#e6db74">&#34;&#34;</span>));
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> { <span style="color:#a6e22e">paths</span> };
</span></span></code></pre></div><p>It just removes <em>&quot;/home/knowledge/&quot;</em> from the start of the path, to match with what <em>darkrag</em> expects.</p>
<p>And the two <em>Http Request</em> nodes:</p>
<p><img src="knowledge-updater-3.png" alt="Knowledge Base Updater Update Request"></p>
<p><img src="knowledge-updater-4.png" alt="Knowledge Base Updater Delete Request"></p>
<p><strong>Knowledge Base Rebuilder</strong></p>
<p>This runs once a week to make doubly sure all the knowledge in the database is up to date.</p>
<p><img src="rebuilder-1.png" alt="Knowledge Base Rebuilder"></p>
<p>First, it deletes files from the database that are no longer present in the knowledge base.</p>
<p><img src="rebuilder-2.png" alt="Knowledge Base Rebuilder Clean Database"></p>
<p>Then, it processes all files in the knowledge base to make sure all the information in the knowledge base is also in the database.</p>
<p><img src="rebuilder-3.png" alt="Knowledge Base Rebuilder Process All"></p>
<p><strong>RAG Webhook</strong></p>
<p>Finally, we configure our actual webhook. It&rsquo;s another pretty simple <em>n8n</em> workflow:</p>
<p><img src="webhook-1.png" alt="RAG Webhook Overview"></p>
<p>It contains only a couple of nodes. Here&rsquo;s the configuration for <strong>Supabase</strong>:</p>
<p><img src="webhook-2.png" alt="RAG Webhook Supabase"></p>
<p><img src="webhook-3.png" alt="RAG Webhook Ollama"></p>
<p>And the code inside the <em>Code</em> node that extracts the knowledge and prepares the request to <strong>darkrag</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">knowledge</span> <span style="color:#f92672">=</span> [];
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">item</span> <span style="color:#66d9ef">of</span> <span style="color:#a6e22e">$input</span>.<span style="color:#a6e22e">all</span>()) {
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">relPath</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">item</span>.<span style="color:#a6e22e">json</span>.document.<span style="color:#a6e22e">metadata</span>.<span style="color:#a6e22e">file_path</span>.<span style="color:#a6e22e">replace</span>(<span style="color:#e6db74">/^\/home\/knowledge\//</span>, <span style="color:#e6db74">&#34;&#34;</span>));
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">content</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">item</span>.<span style="color:#a6e22e">json</span>.document.<span style="color:#a6e22e">pageContent</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">knowledge</span>.<span style="color:#a6e22e">push</span>({
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">file_path</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">relPath</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">content</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">content</span>,
</span></span><span style="display:flex;"><span>  });
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">return</span> { <span style="color:#a6e22e">knowledge</span> }
</span></span></code></pre></div><p>And as a final step, return what <em>darkrag</em> responds with:</p>
<p><img src="webhook-4.png" alt="RAG Webhook Respond"></p>
<h2 id="conclusion">Conclusion</h2>
<p>That&rsquo;s it! Those are all the ingredients to set up your own 100% local, 100% free <em>RAG System</em>. Now when you interact with your favorite model in <em>open-webui</em> the following happens:</p>
<ol>
<li>The <em>open-webui</em> function intercepts your message and sends it to the webhook in <em>n8n</em></li>
<li><em>n8n</em> sends your input to <em>Supabase</em></li>
<li><em>Supabase</em> responds with the 5 most relevant chunks</li>
<li>The <em>open-webui</em> function adds those chunks as context</li>
<li>The LLM responds more accurately to your input</li>
</ol>
<p>And when you add, change, or delete a document from your knowledge base:</p>
<ol>
<li><em>n8n</em> sends the file path to <em>darkrag</em></li>
<li><em>darkrag</em> adds more context by summarizing the chunks, and stores the embeddings of contextualized version, along with the original data, on <em>Supabase</em></li>
</ol>
<p><strong>&ldquo;Does <em>darkrag</em> this solve all problems with RAG?&rdquo;</strong></p>
<p>No, <strong>darkrag</strong> is not a silver bullet. It won’t reliably answer questions like <em>“What happened last week?”</em> or <em>“Rank my achievements by impressiveness.”</em> Instead, it’s a foundation for <em>better chunking and embedding</em>, ensuring retrieved chunks retain the context of their source documents.</p>
<p>Think of <strong>darkrag</strong> as a <em>starting block</em> for building more advanced RAG agents, not a complete solution. You can take the basic <em>n8n</em> setup from this article and extend it into a truly agentic RAG system.</p>
]]></content></item><item><title>RAG Explained</title><link>https://darkbones.dev/posts/explaining-rag/</link><pubDate>Sun, 23 Feb 2025 12:34:51 +0100</pubDate><guid>https://darkbones.dev/posts/explaining-rag/</guid><description>&lt;p>&lt;em>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by enabling them to retrieve relevant information from external sources before generating a response. Because LLMs rely on static training data and do not update automatically, RAG provides a way to integrate fresh, domain-specific, or private knowledge without requiring costly retraining.&lt;/em>&lt;/p>
&lt;p>&lt;em>In this article, we will explore how RAG works, why it is useful, and how it differs from traditional LLM prompting. Expect oversimplified analogies, light technical deep dives, and a few poorly drawn diagrams to tie everything together.&lt;/em>&lt;/p></description><content type="html"><![CDATA[<p><em>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by enabling them to retrieve relevant information from external sources before generating a response. Because LLMs rely on static training data and do not update automatically, RAG provides a way to integrate fresh, domain-specific, or private knowledge without requiring costly retraining.</em></p>
<p><em>In this article, we will explore how RAG works, why it is useful, and how it differs from traditional LLM prompting. Expect oversimplified analogies, light technical deep dives, and a few poorly drawn diagrams to tie everything together.</em></p>
<h2 id="rag">RAG</h2>
<p>Large Language Models excel at many tasks. They can code, draft emails, hallucinate ingredients for the perfect sandwich, and even write articles, although I still prefer doing that myself. However, they have a major limitation. They lack real-time knowledge. Because training LLMs is a time-consuming process, they do not &ldquo;know&rdquo; about recent events. If you ask one about last week, it will either display a disclaimer, provide an outdated answer, or generate something completely inaccurate.</p>
<p>Some LLMs overcome this limitation by retrieving real-time data from external sources before responding. This approach, known as <em>Retrieval-Augmented Generation (RAG)</em>, allows the model to fetch relevant information and incorporate it into the prompt before generating an answer.</p>
<h2 id="rag---oversimplified">RAG - Oversimplified</h2>
<p>But how does RAG actually work? Instead of looking it up ourselves, let’s ask our favorite LLM:
<img src="1-rag-is-a-piece-of-cloth.png" alt="Rag is a piece of cloth"></p>
<p>This is not quite what we were hoping for. No problem, we can ask Bob instead.
<img src="2-bob-rag.png" alt="Asking Bob"></p>
<p>Surprisingly, Bob did not know the answer either, but he was able to retrieve it. Here is what happened:</p>
<ol>
<li>We asked Bob about RAG.</li>
<li>Bob went to the library and asked the librarian for information.</li>
<li>The librarian pointed him to the right aisle.</li>
<li>Bob retrieved the information.</li>
<li>Now Bob sounds like an expert. Thanks, Bob.</li>
</ol>
<p>This breakdown reveals that Bob is effectively functioning as a RAG agent. With that insight, let’s explore exactly how a RAG agent operates.</p>
<h2 id="rag---simplified">RAG - Simplified</h2>
<p>Let&rsquo;s transform our interaction with Bob into an actual RAG system:</p>
<ul>
<li>Bob represents the <strong>RAG system</strong>.</li>
<li>The librarian acts as an <strong>embedder</strong>.</li>
<li>The library functions as a <strong>vector database</strong>.</li>
</ul>
<p><img src="4-rag-system.png" alt="Rag System"></p>
<p>Instead of prompting the LLM directly, the user interacts with the <em>RAG system</em>. The <em>RAG system</em> then forwards the prompt to the <em>embedder</em>, which converts it into a <em>vector</em>. This <em>vector</em> is a numeric representation of the prompt. The idea is that information with similar meaning will have similar vector representations.</p>
<p>This vector allows the system to find relevant information in the vector database. When the vector representation of the user&rsquo;s prompt is sent to the database, it retrieves the most relevant matches.</p>
<p>The <em>RAG system</em> then enhances the user&rsquo;s prompt by including the retrieved information:</p>
<pre tabindex="0"><code>&lt;context&gt;
the information returned from the database
&lt;/context&gt;
&lt;user-prompt&gt;
the user&#39;s original prompt
&lt;/user-prompt&gt;
</code></pre><p>That is the entire process. Retrieve, Augment, and Generate. <strong>RAG</strong>.</p>
<p>However, the system cannot retrieve information that has not been added to the database. How do we store new data? The process is straightforward. Instead of using the vector to find relevant information, the system stores the data along with its vector representation.</p>
<p><img src="3-rag-injest.png" alt="Insert Knowledge"></p>
<p>If you were only interested in the big picture, congratulations. You now understand the core concept. However, if you&rsquo;re a fellow neckbeard, let&rsquo;s talk a bit more about vectors and embedders.</p>
<h2 id="what-is-a-vector">What is a Vector?</h2>
<p>In simple terms, a vector is a set of coordinates that describe how to move from A to B. Look at this graph:</p>
<p><img src="5-look-at-this-graph.jpg" alt="Look at this Graph"></p>
<p>This graph has two dimensions. Each point, <strong>A</strong>, <strong>B</strong>, <strong>C</strong>, and <strong>D</strong>, can be described using a two-number coordinate system. The first number tells us how far to move to the right from the origin (0), while the second number tells us how far to move up. To reach <strong>A</strong>, the vector is <code>[3, 7]</code>. To reach <strong>D</strong>, the vector is <code>[3, 0]</code>.</p>
<p>The same principle applies in three dimensions. To move from your desk to the coffee machine, you must travel a certain distance along the <code>x</code>, <code>y</code>, and <code>z</code> axes, forming a three-number coordinate system.</p>
<p><img src="6-3d-vector.png" alt="3 Dimensions"></p>
<p>Humans struggle to visualize more than three dimensions, but computers do not have this limitation. The math remains the same. Four dimensions? That requires a <em>four-number</em> coordinate system. One hundred dimensions? That requires a <em>100-number</em> coordinate system.</p>
<p><img src="this-is-fine.png" alt="This is Fine"></p>
<p>The embedder I use operates in a coordinate system with 768 dimensions. When you have finished trying to visualize that, we can return to simpler, easy-to-draw, two-dimensional graphs.</p>
<h3 id="why-are-vectors-useful">Why Are Vectors Useful?</h3>
<p>Vectors by themselves are simply n-dimensional coordinates that represent points in n-dimensional space. Their usefulness comes from the information they represent.</p>
<p>In the same way, vectors are coordinates not to places, but to information. A specialized LLM, an <em>embedder</em>, is trained on a large corpus of text to figure out similarities and to place these pieces of information somewhere in n-dimensional space such that similar topics tend to be grouped together. Like, when you go to a social event, you&rsquo;re likely to stick with your friends, colleagues, or at least a group of like-minded people.</p>
<p><img src="7-word2vec.png" alt="Word2Vec"></p>
<p>This graph shows how words that are similar in meaning tend to get grouped together in this n-dimensional space. Modern embedders (like <strong>BERT</strong>) don&rsquo;t use single-word embeddings anymore, but generate <em>contextual embeddings</em>.</p>
<p>The ability to group similar concepts in vector space makes embeddings powerful. However, early embedding models like <em>Word2Vec</em> had a significant limitation that modern models have addressed.</p>
<h4 id="quick-tech-tangent">Quick Tech Tangent</h4>
<p>If you&rsquo;ve been working on AI systems for as long as I have, you might be familiar with <strong>Word2Vec</strong>. While groundbreaking when it came out in 2013, it has a major flaw: it assigns a <strong>single vector</strong> to each word, no matter the context.</p>
<p>Take the word <em>&ldquo;bat&rdquo;</em>.</p>
<ul>
<li>Are we talking about the <strong>flying mammal</strong>? Then it should be near <em>&ldquo;mammal&rdquo;</em>, <em>&ldquo;cave&rdquo;</em>, and <em>&ldquo;nocturnal&rdquo;</em>.</li>
<li>Or do we mean a <strong>baseball bat</strong>? Then it belongs near <em>&ldquo;ball&rdquo;</em>, <em>&ldquo;pitch&rdquo;</em>, and <em>&ldquo;base&rdquo;</em> (but what <em>base</em>? Military?)</li>
<li>And what if we&rsquo;re in the world of <strong>fiction</strong>? Then <em>&ldquo;bat&rdquo;</em> relates to <em>&ldquo;vampire&rdquo;</em> and <em>&ldquo;transformation&rdquo;</em>.</li>
</ul>
<p><strong>Word2Vec</strong> can&rsquo;t tell the difference. It picks one and sticks with it.</p>
<p>One thing I find particularly fascinating with Word2Vec is that, since words are now represented by numbers, you can actually do arithmetic on them.</p>
<p>You can make equations like
<code>king - man + woman = queen</code></p>
<p>It&rsquo;s wild, but it works (most of the time).</p>
<p><strong>Tangent over.</strong></p>
<h3 id="how-are-vectors-used">How are Vectors Used?</h3>
<p>Now that we understand vectors, the next step is straightforward. We embed the information we want the LLM to access, and when we ask a question about that information, the question itself should be close to the relevant content in vector space. The vector database retrieves the <code>n</code> most relevant pieces of content, where <code>n</code> is a configurable number. It also returns the <em>cosine similarity</em> score for each result, indicating how closely the retrieved content matches the query.</p>
<p>Cosine similarity measures the angle between two vectors. A smaller angle indicates greater similarity, meaning the retrieved data is more relevant to the prompt.</p>
<p><img src="8-cosine-similarity.png" alt="Cosine Similarity"></p>
<p>In our example, <em>A</em> and <em>B</em> represent the phrases <em>&ldquo;RAG stands for Retrieval Augmented Generation&rdquo;</em> and <em>&ldquo;Hey LLM, tell me about RAG&rdquo;</em>. Since they are closely related, their vectors are similar. If we instead ask <em>&ldquo;Describe an Eclipse&rdquo;</em>, its vector will be far from the others, making it unrelated. However, if <em>&ldquo;RAG stands for Retrieval Augmented Generation&rdquo;</em> is the only entry in the database, it will still be retrieved, even if it is not relevant to the query.</p>
<h2 id="limitations-of-rag">Limitations of RAG</h2>
<p>Typically, we do not store and retrieve entire documents in the vector database. If we did, a single large document could easily exceed the context window of the LLM. If the system is configured to return the ten most relevant pieces of information, and each of them is the size of a full article, your computer quickly turns into a space heater. To prevent this, we split the information into chunks of a predefined size, such as <code>1000</code> characters, and we try to keep the sentences and paragraphs intact.</p>
<p>However, splitting information into chunks introduces a new problem. Just as <em>Word2Vec</em> struggles to determine meaning from a single word, RAG often fails to understand the full context of a single chunk, especially when that chunk is extracted from the middle of a document.</p>
<p><img src="9-over-rag.png" alt="Rag-time"></p>
<p>Here is a problem I encountered recently. I keep a detailed work diary where I document all my professional achievements. It is extremely useful during performance reviews. However, when I ask my RAG system what I achieved at my current company, it confidently includes accomplishments from my previous jobs. Because I write this diary in the first person and also include information from other sources written in the first person, the system cannot distinguish between them. As a result, it starts attributing achievements to me that I had nothing to do with. That is how I realized something was wrong. My system was suddenly telling me about all the interesting things I supposedly did away from the computer, which is impossible since I never leave my desk.</p>
<p><strong>Want to know how I fixed this mess?</strong> In [the next article], I walk you through how I made my RAG system <strong>context-aware</strong>. You can even steal my code and set it up on your own computer with a couple of commands.</p>
<p>As I learned firsthand, retrieving information <em>does not guarantee understanding</em>. In <a href="/posts/rag-but-i-made-it-smarter/">my next article</a>, I demonstrate how I made my RAG system <strong>context-aware</strong> so that it can truly comprehend the context of the data it retrieves.</p>
<h2 id="conclusion">Conclusion</h2>
<p>RAG makes LLMs more useful by letting them retrieve information they wouldn’t otherwise have access to. But it’s not magic. It comes with its own challenges, from handling context properly to avoiding irrelevant results.</p>
<p>And as I found out the hard way, just because an AI can fetch information doesn’t mean it always understands it. In the next article, I’ll show you how I made my RAG system <strong>context-aware</strong>—so it actually knows what it’s talking about.</p>
]]></content></item><item><title>One Foot in the Unknown</title><link>https://darkbones.dev/posts/one-foot-in-the-unknown/</link><pubDate>Sat, 25 Jan 2025 07:05:51 +0100</pubDate><guid>https://darkbones.dev/posts/one-foot-in-the-unknown/</guid><description>&lt;p>&lt;strong>Maybe it&amp;rsquo;s because I&amp;rsquo;m self-taught and didn&amp;rsquo;t start coding professionally until eight years into my now 13-year journey and I never truly got over feeling like I&amp;rsquo;m &amp;ldquo;not good enough yet to get paid for writing code&amp;rdquo;. Perhaps it&amp;rsquo;s pure passion—or even a touch of masochism. Whatever the reason, I embrace a little chaos in my life.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>No matter how you got here—whether with a CS degree, learning everything on YouTube, attending a bootcamp, or, like me, tinkering on personal projects as if your life depended on it—we all start at the same place: Knowing. Absolutely. Nothing.&lt;/strong>&lt;/p></description><content type="html"><![CDATA[<p><strong>Maybe it&rsquo;s because I&rsquo;m self-taught and didn&rsquo;t start coding professionally until eight years into my now 13-year journey and I never truly got over feeling like I&rsquo;m &ldquo;not good enough yet to get paid for writing code&rdquo;. Perhaps it&rsquo;s pure passion—or even a touch of masochism. Whatever the reason, I embrace a little chaos in my life.</strong></p>
<p><strong>No matter how you got here—whether with a CS degree, learning everything on YouTube, attending a bootcamp, or, like me, tinkering on personal projects as if your life depended on it—we all start at the same place: Knowing. Absolutely. Nothing.</strong></p>
<h2 id="phase-0-knowing-nothing">Phase 0: Knowing Nothing</h2>
<p><img src="i_understand_nothing.png" alt="I Understand Nothing"></p>
<p>This phase is as uncomfortable as it is frustrating. One moment, you&rsquo;re on the verge of giving up; the next, you&rsquo;re desperately waiting for that breakthrough when everything finally clicks. Sometimes, you even experience both at once.</p>
<p>The good news is that you don&rsquo;t have to stay in that place forever. The better news is that you can.</p>
<h2 id="phase-1-knowing-something">Phase 1: Knowing &ldquo;Something&rdquo;</h2>
<p><img src="patrick_building.jpg" alt="Knowing Something"></p>
<p>At first, you learn to do something very basic—like writing a program that prints &ldquo;Hello World!&rdquo; to a console. It&rsquo;s a small win, but it&rsquo;s your first taste of what coding can do.</p>
<p>Then, sooner or later, you get an opportunity to put that simple skill to work. Maybe it&rsquo;s a task from your boss, a class assignment, or a requirement in your personal project: printing a line of text to a console. When you get to do that task, your heart leaps with joy. You create a new branch, implement the solution, push it to the repo, and suddenly, everyone can see that little victory and you only turned to Google twice. In that moment, you feel like a real engineer.</p>
<p>I remember feeling like I was on top of the world—at the summit of mount stupid—after learning about <code>if</code> statements. I thought, &ldquo;Programming is just about writing a bunch of <code>if</code> statements to decide whether to do <code>x</code> or <code>y</code>.&rdquo;</p>
<p>But, alas, not every task is as simple as printing a line of text or toggling between <code>x</code> and <code>y</code>.</p>
<h2 id="phase-2-the-perceived-valley-of-despair">Phase 2: The Perceived Valley of Despair</h2>
<p><img src="valley_of_dispair.png" alt="The Valley of Dispair"></p>
<p>Eventually, all who reach the summit of mount stupid must take that inevitable slide down into the valley of despair.</p>
<p>As you tackle more tasks and get better at suppressing the thoughts of giving up, you also discover more layers to the craft. You become partly capable of handling your job or project, yet with every new concept mastered, five more emerge that you <strong>previously</strong> didn&rsquo;t know existed. Upon realizing each of those five things come with their own five things you <strong>currently</strong> don&rsquo;t know the existence of, the thoughts of quitting might grow a little stronger.</p>
<blockquote>
<p>Turns out, a good application is more than a bunch of <code>if</code> statements. Who knew?</p></blockquote>
<p>Every question you ask only seems to spawn more questions. If you&rsquo;re fortunate enough to have a mentor during this phase, you might worry that you&rsquo;re asking too much—fearing you&rsquo;re becoming a burden. And if you&rsquo;re lucky enough to have a good mentor, rest assured, they&rsquo;ve been there too.</p>
<p>It might seem similar to Phase 1, but don&rsquo;t be fooled. This is a whole different beast. Here, you stand with one foot firmly in the chaos of the unknown and the other planted in the comfort of the familiar.</p>
<blockquote>
<p>I know how to write this <code>if</code> statement. And I know how to write a class. But where do I put it? And how do I import it in the other class? And why is my PR rejected with a comment about &rsquo;early returns&rsquo;? And what is an &rsquo;early return&rsquo;? Why do we care about &lsquo;performance&rsquo; so much? It seems fast enough when I run it on my machine&hellip;</p></blockquote>
<h2 id="phase-3-knowing-almost-all-the-things">Phase 3: Knowing Almost All The Things</h2>
<p><img src="bateman.gif" alt="Bateman Walking"></p>
<p>While you’re busy nurturing the foot that’s still in chaos, you might not notice that the area of the familiar is quietly expanding beneath the other. And before you know it, you’re firmly planted in comfort. Welcome to being: &ldquo;A pretty good engineer&rdquo;. You might feel a sense of pride—after all, new tasks no longer fill you with dread because you believe you can handle almost anything the job throws at you.</p>
<p>Sure, you’ll still occasionally be stumped by something new or confusing. But hey, who isn’t?</p>
<blockquote>
<p>Infrastructure is just messy and confusing. Fortunately, we have an infrastructure guy on the team so I never have to worry about setting up Helm charts or poking around in AWS. That&rsquo;s what infrastructure guy™ is for!</p></blockquote>
<p>A sense of peace—one you never realized you never wanted—begins to settle in.</p>
<h2 id="phase-4-knowing-more-than-you-need">Phase 4: Knowing More Than You Need</h2>
<p><img src="knowing_things.jpg" alt="Knowing Things"></p>
<p>Of course, production goes down on the same day that infrastructure guy™ starts his 4-week PTO. &ldquo;Okay, no need to panic,&rdquo; you tell yourself. &ldquo;What did he say during standup the other day? Something about a Kubernetes cluster? I&rsquo;ll take a look.&rdquo;</p>
<p>Great, our sales department has an exciting lead for a big Swiss customer! Wait, what&rsquo;s that? They&rsquo;re required by law to have all their data in Switzerland? But our database is&hellip; in the Cloud? Unless that Cloud is permanently hanging over Zurich, we need to either move our data to Switzerland or set up a separate database there and configure the app to somehow use this other database. Where&rsquo;s infrastructure guy™? Oh right, that was two jobs ago.</p>
<p>Even though you no longer see yourself as being in a &ldquo;learning phase,&rdquo; knowledge continues to accumulate through adversity. You’ve moved from &ldquo;knowing enough to handle your daily tasks&rdquo; to knowing way more than that.</p>
<blockquote>
<p>I&rsquo;m <strong>still</strong> required to learn things? This is BS! I should be cruising, adding years of experience, salary bumps, and fancy titles!</p></blockquote>
<p>That creeping sense of peace eventually turns to dread. Did I lose my passion for tech? Did I trade dissociating at my desk in a call center for dissociating at my desk in a tech firm? Is this really what I worked so dang hard for? What am I going to do for the next 30 years of my life? Is it CRUD operations forever?</p>
<h2 id="deceived-by-comfort">Deceived by Comfort</h2>
<p><img src="tech_is_big.png" alt="Tech Is Big"></p>
<p>Years ago, while mentoring a small team of junior developers, I began to envy how much they were still learning. I found myself wishing I could go back to that exhilarating “Phase 2”—when you’re in the zone, challenged enough to notice improvement from one week to the next.</p>
<p>I thought, if mentoring juniors is the final destination, I might as well get really good at it. So I turned to YouTube for inspiration and ended up finding tech influencers like <a href="https://www.youtube.com/@ThePrimeTimeagen">ThePrimeagen</a>, <a href="https://www.youtube.com/@teej_dv">tjdevries</a>, and <a href="https://www.youtube.com/@typecraft_dev">typecraft</a>. They’re still burning with passion for programming. Where does that spark come from?</p>
<p>I saw them using tools like Neovim and Arch Linux (I now also use Arch Linux, by the way), advocating for proper typing, and relentlessly learning new things. And that’s when it hit me—I could be just like those junior engineers. And I could <em>start today</em>.</p>
<h2 id="phase-2-the-perceived-valley-of-despair-again">Phase 2: The Perceived Valley of Despair. Again.</h2>
<p><img src="more_dispair.png" alt="More Dispair"></p>
<p>You do not know all the things. Nobody in tech knows everything—it’s impossible to know it all. That&rsquo;s not a bug; it&rsquo;s a feature.</p>
<p>If you&rsquo;re bored with the comfort of familiarity, all you have to do to get back into Phase 2 is <em>move your feet</em>.</p>
<p>But what can you do when you&rsquo;re tired of adding yet another set of CRUD endpoints to an API?</p>
<h3 id="treat-it-like-a-video-game">Treat It Like A Video Game</h3>
<p>What do you do when you get bored of a video game but can&rsquo;t find something else to play? Try one or more of these:</p>
<ul>
<li><strong>Speedrun it.</strong></li>
<li><strong>Use a different editor.</strong></li>
<li><strong>Install mods (a.k.a. Neovim plugins).</strong></li>
<li><strong>Make your own mods.</strong></li>
<li><strong>Automate it.</strong></li>
<li><strong>Rewrite it in Rust.</strong></li>
<li><strong>Do it branchless without a single <code>if</code> statement.</strong> (Phase 1 me would have cried at the thought.)</li>
<li><strong>Become infrastructure guy™.</strong></li>
</ul>
<p>It also helps to be mindful about tiny frustrations that come up from time to time. Typing a long command in your terminal and realize you made a typo at the very start? You can either hold the left arrow key for five minutes or learn how to get to a specific part of a command more efficiently in the same amount of time.</p>
<h3 id="do-something-else-entirely">Do Something Else Entirely</h3>
<p>Are you a frontend engineer? Try backend. A backend engineer? Experiment with frontend. Full-stack? Build a video game. A game engineer? Create a text editor. For that matter, consider:</p>
<ul>
<li>Git gud at LeetCode.</li>
<li>Learn to type faster.</li>
<li>Build an embedded system.</li>
<li>Learn Assembly.</li>
<li>Start giving tech talks at work.</li>
<li>Find out what BTRFS is and how it works.</li>
<li>Experiment with writing a graphics shader.</li>
<li>Build an AI using nothing but LSTM, feed it Christmas songs in LilyPond format, and play the generated tunes at your Christmas dinner party to the annoyance of your guests.</li>
<li>Contribute to open source projects.</li>
<li>Participate in a data science competition at CERN with LHC data (I got surprisingly far).</li>
<li>Automate your house—make it track how many Chocolate Peanut Butter Oreos you have in the cupboard so you never run out again.</li>
</ul>
<p>It&rsquo;s okay to fail at half of these things. It&rsquo;s okay to only attempt a few. The only way to lose is by becoming too comfortable for too long.</p>
<h3 id="heading"><em>&ldquo;Is it just learning forever?&rdquo;</em></h3>
<p>Yes.</p>
<h3 id="heading-1"><em>&ldquo;What if I get tired of learning?&rdquo;</em></h3>
<p>If you get tired of having one foot in the known and the other in the unknown, be kind to yourself. Take a little break. A little vacation, if you will. It&rsquo;s okay. Everyone needs a break sometimes and you deserve it. Put <strong>both</strong> feet in the unknown. Go back to Phase 0. Switch to DVORAK, install Arch on your MacBook, fire up Emacs and do last year&rsquo;s Advent of Code in Rust with it after putting ChatGPT in the blocklist of your pi-hole. Embrace the suck. See how fast you can get to Phase 2 again with that setup. Do that before considering taking a vacation in the place of boredom.</p>
<p><img src="pure_unfiltered_suffering.gif" alt="Pure Unfiltered Suffering"></p>
<h2 id="the-takeaways">The Takeaways</h2>
<p>One of the greatest perks of being a knowledge worker is that our job lets us constantly learn cool new things. If you&rsquo;re an engineer—or any kind of knowledge worker—you&rsquo;ve already traveled the journey from knowing nothing to where you are now. Maybe you&rsquo;re content to coast until retirement, and that&rsquo;s perfectly fine. But for me, coasting leads to boredom, and boredom leads to burnout.</p>
<p>As knowledge workers, knowledge is value, and we share that value freely and willingly. When I submit a PR that I poured my heart into—only to have a colleague suggest an obviously better strategy that sends me back to square one—I&rsquo;m thankful every single time. I now know one more thing and that&rsquo;s valuable.</p>
<p>The goal isn’t to know everything—it’s to enjoy not knowing and keep learning anyway.</p>
<p>Even though some might see me as some kind of wizard (my monitor seems to flicker with code, windows pop in and out of existence until I miraculously raise a PR while my hands never left the keyboard), I still get overwhelmed when I look at the giant, ever-growing list of things I still want to learn. And that, in itself, makes me happy.</p>
<p><img src="happy.png" alt="Happy Not Knowing"></p>
]]></content></item></channel></rss>